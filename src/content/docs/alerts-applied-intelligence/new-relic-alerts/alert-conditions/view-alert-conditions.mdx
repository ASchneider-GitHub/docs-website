---
title: Create and edit alert conditions in the UI
tags:
  - Alerts and applied intelligence
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition/
  - /docs/alerts/alert-policies/configuring-alerts/managing-your-alerts 
freshnessValidatedDate: never
---
import alertsSetaQuery from 'images/alerts_screenshot-crop_set-a-query.webp'

import alertsFineTuneAlertSignals from 'images/alerts_screenshot-crop_fine-tune-alert-signals.webp'

import alertsSetaThresholdforanAlertCondition from 'images/alerts_screenshot-crop_set-a-threshold-for-an-alert-condition.webp'

import alertsNameYourAlertCondition from 'images/alerts_screenshot-crop_name-your-alert-condition.webp'

import alertsAlertConditionPage from 'images/alerts_screenshot-crop_alert-condition-page.webp'

Alert conditions can elevate the quality of your alerts. An alert condition contains certain parameters or thresholds that need to be met before you're informed. They can mitigate excessive alerting, or let your team know when new or unusual behavior appears. The **Alert conditions details** page is the universal hub for all of your alert conditions. 

With our **Alert conditions details** page you can:
* Browse all of your alert conditions 
* Easily identify any open issues
* Open existing alert conditions to edit thresholds or signal behavior 
* Create a new alert condition 
* Preview the condition as Terraform or NerdGraph code

<img
    title="alert condition homepage"
    alt="A screenshot showing the alert condition homepage"
    src={alertsAlertConditionPage}
/>
<figcaption>
To use New Relic's universal alert condition experience, go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**, select **Alert Conditions** in the left navigation.
</figcaption>

This doc will walk you through the **Alert conditions details** page and show you:

* How to edit [an _existing_ alert condition](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#edit-existing-alert-condition)
* How to create a [_new_ alert condition](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#create-alert-condition])
* How to preview the condition as Terraform or NerdGraph code.

## Create a new alert condition [#create-alert-condition]

An alert condition is a continuously running query that measures a given set of events against a defined threshold, and opens an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) when the threshold is met for a defined window of time. 

This example shows you how to manually create a new alert condition using the **Alert condition details** page. But there are a lot of ways to create an alert condition. You can create an alert condition from:

* [A chart](/docs/tutorial-create-alerts/create-an-alert/)
* [A policy page](https://one.newrelic.com/nr1-core/condition-builder/policy-entity)
* The condition list page
* Some uncovered entity pages
* [The **Alert coverage gaps** page](https://one.newrelic.com/alerts-ai/detection-gaps/)

You can also use one of our alert builders:
* Use **Write your own query** to build alerts from scratch
* Start from a recommended alert condition
* **Use guided mode** 

For all methods except for our guided mode, the process for creating an alert condition will be _exactly_ the same as described in the steps below. 

<Steps>
<Step>
## Set your initial query 
<CollapserGroup>
  <Collapser
    id="set-your-query"
    title="Set your query"
  >
<img
    title="set a query"
    alt="A screenshot showing a user how to set a query for an alert condition"
    src={alertsSetaQuery}
/>

To create a new alert condtion:

* Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**
* Select **Alert Conditions** in the left navigation.
* Then select **New alert condition**.
* Select **Write your own query**. 

You can use a NRQL query to define the signals that you want an alert condition to use as the foundation for your alert. For this example, we will be using this query:

```
SELECT average(duration) 
FROM PageView 
WHERE appName = 'WebPortal'
```
Using this query for your alert condition tells New Relic that you want to know the average `pageviews` from your `WebPortal` application. Monitoring `pageviews` can help you look out for any latency issues in your application. 

You can learn more about how to use NRQL, New Relic's query language, see our [NRQL documentation](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/). 
</Collapser>
</CollapserGroup>

</Step>

<Step>
## Fine-tune advanced signal settings [#advanced-signal-settings]

After you've defined your signal, click **Run**. A chart will appear and display the parameters that you've set. For this example, the chart with show the average `pageviews` for your `WebPortal` application. 

Click **Next** and begin configuring your alert condition. 

For this example, you're going to customize these advanced signal settings for your condition that we created to monitor an unusual activity for `pageviews` in your `WebPortal` application. 

<img
    title="fine tune alert settings"
    alt="A screenshot depicting advanced signal settings"
    src={alertsFineTuneAlertSignals}
/>

<CollapserGroup>
  <Collapser
    id="window-duration"
    title="Window duration"
  >

The window duration defines how New Relic groups your data for analysis in an alert condition. Choosing the right setting depends on your data's frequency and your desired level of detail:

* **High-frequency data (e.g., pageviews every minute)**: Set the window duration to match the data frequency (1 minute in this case) for real-time insights into fluctuations and trends.
* **Low-frequency data (e.g., hourly signals)**: Choose a window duration that captures enough data to reveal patterns and anomalies (e.g., 60 minutes for hourly signals).

Remember, you can always customize the window duration based on your specific needs and experience. We recommend using the defaults when starting out and experimenting as you become more comfortable with creating alert conditions.
  </Collapser>

  <Collapser
    id="sliding-window"
    title="Use sliding window aggregation"
  >

  Throughout the day, data streams from your application into New Relic. Instead of evaluating that data immediately for incidents, alert conditions collect the data over a period of time known as the **aggregation window**. An additional delay allows for slower data points to arrive before the window is aggregated.

  Sliding windows are helpful when you need to smooth out "spiky" charts. One common use case is to use sliding windows to smooth line graphs that have a lot of variation over short periods of time in cases where the rolling aggregate is more important than aggregates from narrow windows of time.

  We recommend using our sliding window aggregation if you're not expecting to have a steady and consistent stream of data but are expecting some dips and spikes in data. 

  You can learn more about sliding window aggregation in [this NRQL tutorial](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-with-sliding-windows/) or by watching [this video](https://www.youtube.com/watch?v=-5--8DZynFE).
  </Collapser>
  
  <Collapser
    id="streaming-method"
    title="Streaming method"
  >
  In general, we recommend using the **event flow** streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where **event timer** might be a better method to choose, but for your first alert we recommend our default, **event flow**. To better understand which streaming method to choose, we recommend watching this brief video.  

  <Video
  type="wistia"
  id="n6nei987ln"
/>

  </Collapser>

    <Collapser
    id="delay"
    title="Delay"
  >

The delay feature protects you against inconsistent data collection. It gives the alert condition a little wiggle room before deciding to create an incident. If in any given minute your data arrives at New Relic with timestamps from only a single minute, then a low delay setting is enough. On the other hand, if during that minute New Relic receives data points with timestamps from several minutes past or several minutes forward, then your signal is more inconsistent and will need a higher delay setting.

  </Collapser>

  <Collapser
    id="gap-filling-strategy"
    title="Gap-filling strategy"
  >
  We're creating an alert condition to notify our team of any latency issues with the `WebPortal` application. In this example, our application consistently sends New Relic data. There is a constant stream of signals being sent from our application to New Relic and there is no expected gap in signal so we won't need to select a gap-filling strategy. We generally recommend leaving the **gap-filling strategy** set to **none**. 

  If you have a more inconsistent data set sending signals to New Relic once every twenty-four hours, then we'd recommend customizing this feature based on your team's specific needs. 

  Learn more about gap-filling strategies in our [lost signal docs](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).

  </Collapser>
</CollapserGroup>

</Step>

<Step>
## Set thresholds for alert conditions [#thresholds]

If an alert condition is a container, then thresholds are the rules that each alert condition must follow. As data streams into your system, the alert condition searches for any incidents of these rules. If the alert condition sees data coming in from your system that has met all the conditions you've set, then it will create an incident. An incident is a signal that something is off in your system and you should take a look. 

<img
    title="set a threshold"
    alt="A screenshot depicting how to set threshold for an alert condition "
    src={alertsSetaThresholdforanAlertCondition}
/>

<CollapserGroup>
  <Collapser
    id="anomaly-threshold"
    title="Anomaly threshold (recommended)"
  >

  You are creating this alert condition so you'll be notified if there are fewer `pageviews` than usual for your `WebPortal` application. But let's say you don't care about how the exact number of `pageviews` your `WebPortal` applications is getting, and you just want to know if transaction time is behaving abnormally. For this specific use case, we'd recommend using our **anomaly threshold**. Our anomaly detection constantly evaluates your data to understand how your system normally behaves. By setting an anomaly threshold, you can use our anomaly detection to alert your team if `pageviews` deviate from their expected performance. Since you only want to know if `pageviews` are behaving unusually, you'd select **upper and lower** because you want notifications of _all_ deviations. But, if you only want to know if `pageviews`are _higher_than usual, you'd select **upper only.**

  Next, you need to set the priority level. The priority level determines what will create an incident. We recommend setting your priority level as **critical** for your first alert. You can learn more about priority levels in our [alert condition docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels).

  Next, you must choose what defines a **critical anomaly threshold breach**. For this first alert, we recommend using our default settings and adjusting to your needs as necessary. So, leave the settings to open an incident "when a query returns a value deviates from the predicted value: **for at least five minutes** by **3 standard deviations**".

  Learn more about anomalies in our anomaly [documentation](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/).

</Collapser>

  <Collapser
    id="static-threshold"
    title="Static threshold"
  >
    Unlike anomaly thresholds, a static threshold doesn't look at your data set as a whole and determines what behavior is unusual based on your system's history. Instead, a static threshold will open an incident whenever your system behaves differently than the criteria that **you set**. Static alert thresholds are much more customizable, and we recommend them if you've a strong sense of your data and what you're looking for. 

    Learn more about our static alert conditions in our [NRQL docs](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#threshold-types).
  </Collapser>
  
  <Collapser
    id="lost-signal"
    title="Lost signal threshold (optional)"
  >

The loss signal threshold determines how long to wait before considering a missing signal lost. If the signal doesn't return within that time, you can choose to open a new incident or close any related open ones. Set the threshold based on your system's expected behavior and data collection frequency. For example, if a lost signal for `pageviews` could indicate a latency issue, set a threshold you're comfortable with and check the box to open a new lost signal incident.

  </Collapser>
</CollapserGroup>
</Step>

<Step>

## Additional details [#additional-details]

  At this point in the process we now have a fully defined container and we've set all the rules to make sure an incident is opened when we want it to be. Based on the settings above, if our alert condition recognizes this behavior in our system that breaches the thresholds that we've set, it will create an incident. Now, all we need to do is to name this container and attach it to a policy. 
  
  The policy is the sorting system for the incident. When you create a policy, you're creating the tool that organizes all of your incoming incidents. You can connect policies to **[workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/)** that tell New Relic where you want all this incoming information to go, how often you want it to be sent, and where. 

<img
    title="name an alert condition "
    alt="A screenshot demonstrating how you can new alert condition "
    src={alertsNameYourAlertCondition}
/>

<CollapserGroup>
  <Collapser
    id="name-your-condition"
    title="Name your condition"
  >
    It's important to give your alert condition a descriptive name. Let's say you name this condition **pageviews** and then you create another condition for a completely different application and label that condition **pageviews** as well. If this occurs you won't be able to distinguish which condition is for which application. So, make sure to give your condition a specific and unique name. In this case, we'd name this condition **pageviews: WebPortal App**.
  </Collapser>

  <Collapser
    id="select-an-existing-policy"
    title="Select an existing policy"
  >

  If you already have a policy you want to connect to an alert condition, then select the existing policy. 

  Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>

  <Collapser
    id="create-a-new-policy"
    title="Create a new policy"
  >
    If we want to create a new policy for this alert condition, here's our chance. Remember, a policy is connected to workflows and workflows control how often we're notified about any incidents. It's a fine balance between ensuring that we learn about any issues with `pagviews` on our `WebPortal` application as quickly as possible and making sure that we don't get so many alerts that our developers experience fatigue and start missing out on important information because of information overload. 

    Policies can hold one or multiple conditions. If you're looking to monitor `pageviews`, you have a few options.
    
    First, you could create a policy that only attaches a single issue per policy (the default option). One issue per policy reduces noise but also requires immediate action. But this means that if you've attached multiple conditions to this policy, not just to **Response time: Example app**, then no matter what, all incidents in this policy will be grouped into one single issue.

    Or we could create one issue per condition. This means that any time the **Response time: Example app** condition opens an incident, all those incidents will be rolled into one issue that is connected to our condition. For this specific use case, you should choose this option because it meets the primary goal, which is to monitor latency issues with `pageviews`. 

    Or we could create an issue for every incident. This option is the most noisy but can work well if you like to send information to an external system. 

    Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>
  <Collapser
    id="close-open-incidents"
    title="Close open incidents"
  >
  An incident will automatically close when the targeted signal returns to a non-breaching state for the time period indicated in the condition's thresholds. This wait time is called the recovery period.

  For example, if the breaching behavior is "`pageviews` are lower than 300 at least once in 5 minutes" then the incident will automatically close when `pageviews` are equal to or higher than 300 for 5 consecutive minutes. 

  When an incident closes automatically:

  1. The closing timestamp is backdated to the start of the recovery period.
  2. The evaluation resets and restarts from when the previous incident ended.

  All conditions have an incident time limit setting that will automatically force-close a long-lasting incident.

  We automatically default to 3 days and recommend that you use our default settings for your first alert. 
  </Collapser>

  <Collapser
    id="custom-incident-description"
    title="Send a custom incident description"
  >
  
  Since we're creating an alert condition that lets us know if there are any latency issues with our `WebPortal` application, we want to make sure our developers have all the information they need when they are notified about this incident. We're going to use workflows to notify a team Slack channel when an incident is created.

  Learn more about custom incident descriptions in our [docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-incident-descriptions).

</Collapser>

  <Collapser
    id="runbook"
    title="Add runbook url"
  >
If you'd like to link to a runbook, you can put the URL in the runbook URL field.
  </Collapser>
</CollapserGroup>
</Step>
</Steps>

## Edit or improve an existing alert condition [#edit-existing-alert-condition]

If you want to edit an alert condition you've already created, you can:

1. Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**.
2. Select **Alert Conditions** in the left navigation.
3. Click on the alert condition you want to edit.

From there, you will be able to see all of the elements you set when creating your condition. You can edit specific elements of the alert condition by clicking the pencil in top right of each section. 

If you have questions about how to edit each section, refer back to `Create a new alert condition` section:

* [Signal coverage](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#create-alert-condition)
* [Signal behavior](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#advanced-signal-settings)
* [Thresholds](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#thresholds)
* [Additional details](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#additional-details) 

### Signal history [#signal-coverage]

Signal coverage gives a closer look at the most recent signals being ingested by the condition. For Synthetics type conditions, a chart is shown with the 5 most recent `SyntheticChecks`. For all other types, a line chart showing the `NraiSignal` over time for the last few days is shown for easier analysis. This chart is similar to what is shown for an individual issue or incident.

If a condition is disabled, you’ll see an empty state message in this section of the Overview. In the case where the condition is not receiving a signal, you will see an empty state prompting you to adjust your condition settings.

### Troubleshoot your signal history chart
If you see an empty signal in the history chart, consider one of the following: 

* **Review the condition's settings**: Double-check that all elements are configured correctly.
* **Inspect NRQL queries**: Ensure they're targeting valid metrics or events and returning data.
* **Examine entity configuration**: Confirm that the entity is set up properly to send signals.
* **Consult New Relic documentation**: Refer to relevant guides for assistance with specific issues.


## What's next? [#whats-next]
<DocTiles>
  <DocTile title='Get started with New Relic alerts' path="/docs/tutorial-create-alerts/create-new-relic-alerts/"> Start here if you're a beginner to New Relic alerts</DocTile>
  <DocTile title='Advanced alert conditions' path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/"> If you've already set up your alert conditions, dig deeper with advanced settings</DocTile>
  <DocTile title='Use NRQL conditions' path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/"> Use NRQL to create alert conditions</DocTile>
</DocTiles>




