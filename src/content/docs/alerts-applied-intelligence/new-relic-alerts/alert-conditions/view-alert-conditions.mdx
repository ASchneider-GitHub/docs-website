---
title: Create and edit alert conditions in the UI
tags:
  - Alerts and applied intelligence
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition/
  - /docs/alerts/alert-policies/configuring-alerts/managing-your-alerts 
freshnessValidatedDate: never
---
import alertsSetaQuery from 'images/alerts_screenshot-crop_set-a-query.webp'

import alertsFineTuneAlertSignals from 'images/alerts_screenshot-crop_fine-tune-alert-signals.webp'

import alertsSetaThresholdforanAlertCondition from 'images/alerts_screenshot-crop_set-a-threshold-for-an-alert-condition.webp'

import alertsNameYourAlertCondition from 'images/alerts_screenshot-crop_name-your-alert-condition.webp'

import alertsAlertConditionPage from 'images/alerts_screenshot-crop_alert-condition-page.webp'

New Relic alerts are built with alert conditions. If you want to be notified of any unusual behavior in your stack, you'll first need to create an alert condition. An alert condition is essentially a container that you create to define what conditions need to be met before you're notified of any unusual behavior. 

<img
    title="alert condition homepage"
    alt="A screenshot showing the alert condition homepage"
    src={alertsAlertConditionPage}
/>
<figcaption>
To use New Relic's universal alert condition experience, go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**, select **Alert Conditions** in the left navigation.
</figcaption>

With our **Alert Conditions** page you can:
* Browse all of your alert conditions 
* Easily identify any open issues
* Open existing alert conditions to edit thresholds or signal behavior 
* Create a new alert condition 

This doc will walk you through how to edit [an _existing_ alert condition](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#edit-existing-alert-condition) and how to create a [new_ alert condition_](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#create-alert-condition]). 

## Create a new alert condition [#create-alert-condition]

For this example, we are going to show you how to manually create a new alert condition using the **Alert Condition** UI. But, if you're just beginning to use New Relic, we suggest creating an alert condition from a chart. To get started with alerts, review our [tutorial about creating your first alert](/docs/tutorial-create-alerts/create-an-alert/). 

<Steps>
<Step>
An alert condition is essentially a container that you create to define what conditions need to be met before you're notified of any unusual behavior.

<img
    title="set a query"
    alt="A screenshot showing a user how to set a query for an alert condition"
    src={alertsSetaQuery}
/>

<figcaption>
To create a new alert, go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**, select **Alert Conditions** in the left navigation, then select **New alert condition**.
</figcaption>

<CollapserGroup>
  <Collapser
    id="set-your-query"
    title="Set your query"
  >

For this example, we will be using this query:

```
SELECT average(duration) 
FROM PageView 
WHERE appName = 'WebPortal'
```
Using this query for your alert condition tells New Relic that you want to know the average pageviews from your `WebPortal` application. Monitoring `pageviews` can help you look out for any latency issues in your application. 

You can learn more about how to use NRQL, New Relic's query language, see our [NRQL documentation](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/). 

</Collapser>
</CollapserGroup>
</Step>

<Step>
## Fine-tune advanced signal settings [#advanced-signal-settings]

New Relic constantly observes the data that streams from your applications and into our system. But not all applications send signals at the same frequency or cadence. Some events could send signals to our system once a minute while others could only report data to New Relic once every day. An alert condition is a **specific** container designed for a **specific** use case. When creating an alert condition this section is the most customizable for the data that you're evaluating. 

We're going to customize these advanced signal settings for our condition that is montioring `pageviews` in the `WebPortal` application. 

<img
    title="fine tune alert settings"
    alt="A screenshot depicting advanced signal settings"
    src={alertsFineTuneAlertSignals}
/>

<CollapserGroup>
  <Collapser
    id="window-duration"
    title="Window duration"
  >

  Setting the window duration for your alert condition tells New Relic how to group your data. If you're creating an alert condition for a data set that sends a signal to New Relic once every hour, you'd want to set the window duration to something closer to sixty minutes because it'll help spot patterns and unusual behavior. But, if you're creating an alert condition to monitor `pageviews` on your `WebPortal` application and New Relic collects a signal for that data every minute, we'd recommend setting the window duration to one minute. 
  
  When you're getting started we recommend sticking with our default settings, but the more you get familiar with creating an alert condition we encourage you to customize these fields based on your own experience. 
  </Collapser>

  <Collapser
    id="sliding-window"
    title="Use sliding window aggregation"
  >

  Throughout the day, data streams from your application into New Relic. Instead of evaluating that data immediately for incidents, alert conditions collect the data over a period of time known as the **aggregation window**. An additional delay allows for slower data points to arrive before the window is aggregated.

  Sliding windows are helpful when you need to smooth out "spiky" charts. One common use case is to use sliding windows to smooth line graphs that have a lot of variation over short periods of time in cases where the rolling aggregate is more important than aggregates from narrow windows of time.

  We recommend using our sliding window aggregation if you're not expecting to have a steady and consistent stream of data but are expecting some dips and spikes in data. 

  You can learn more about sliding window aggregation in [this NRQL tutorial](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-with-sliding-windows/) or by watching this video.

  <Video
    type="youtube"
    id="-5--8DZynFE"
  />

  </Collapser>
  
  <Collapser
    id="streaming-method"
    title="Streaming method"
  >
  In general, we recommend using the **event flow** streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where **event timer** might be a better method to choose, but for your first alert we recommend our default, **event flow**. To better understand which streaming method to choose, we recommend watching this brief video.  

  <Video
  type="wistia"
  id="n6nei987ln"
/>

  </Collapser>

    <Collapser
    id="delay"
    title="Delay"
  >

The delay feature protects you against inconsistent data collection. It gives the alert condition a little wiggle room before deciding to create an incident. If in any given minute your data arrives at New Relic with timestamps from only a single minute, then a low delay setting is enough. On the other hand, if during that minute New Relic receives data points with timestamps from several minutes past or several minutes forward, then your signal is more inconsistent and will need a higher delay setting.

  </Collapser>

  <Collapser
    id="gap-filling-strategy"
    title="Gap-filling strategy"
  >
  We're creating an alert condition to notify our team of any latency issues with the `WebPortal` application. In this example, our application consistently sends New Relic data. There is a constant stream of signals being sent from our application to New Relic and there is no expected gap in signal so we won't need to select a gap-filling strategy. We generally recommend leaving the **gap-filling strategy** set to **none**. 

  If you have a more inconsistent data set sending signals to New Relic once every twenty-four hours, then we'd recommend customizing this feature based on your team's specific needs. 

  Learn more about gap-filling strategies in our [lost signal docs](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).

  </Collapser>
</CollapserGroup>

</Step>

<Step>
## Set thresholds for alert conditions [#thresholds]

If an alert condition is a container, then thresholds are the rules that each alert condition contains. As data streams into your system, the alert condition searches for any incidents of these rules. If the alert condition sees data coming in from your system that has met all the conditions you've set, then it will create an incident. An incident is a signal that something is off in your system and you should take a look. 

<img
    title="set a threshold"
    alt="A screenshot depicting how to set threshold for an alert condition "
    src={alertsSetaThresholdforanAlertCondition}
/>

<CollapserGroup>
  <Collapser
    id="anomaly-threshold"
    title="Anomaly threshold (recommended)"
  >

  Your team is creating this alert condition so you'll be notified if there are fewer `pageviews` than usual. But let's say you don't care about how the exact number of `pageviews` your `WebPortal` applications is getting, and you just want to know if transaction time is behaving abnormally. For this specific use case, we'd recommend using our **anomaly threshold**. Our anomaly detection constantly evaluates your data to understand how your system normally behaves. By setting an anomaly threshold, you can use our anomaly detection to alert your team if `pageviews` deviate from their expected performance. Since you only want to know if `pageviews` are behaving unusually, you'd select **upper and lower** because you want notifications of _all_ deviations. But, if you only want to know if `pageviews`are _higher_than usual, you'd select **upper only.**

  Next, you need to set the priority level. The priority level determines what will create an incident. We recommend setting your priority level as **critical** for your first alert. You can learn more about priority levels in our [alert condition docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels).

  Next, you must choose what defines a **critical anomaly threshold breach**. For this first alert, we recommend using our default settings and adjusting to your needs as necessary. So, leave the settings to open an incident "when a query returns a value deviates from the predicted value: **for at least five minutes** by **3 standard deviations**".

  Learn more about anomalies in our anomaly [documentation](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/).

</Collapser>

  <Collapser
    id="static-threshold"
    title="Static threshold"
  >
    Unlike anomaly thresholds, a static threshold doesn't look at your data set as a whole and determines what behavior is unusual based on your system's history. Instead, a static threshold will open an incident whenever your system behaves differently than the criteria that **you set**. Static alert thresholds are much more customizable, and we recommend them if you've a strong sense of your data and what you're looking for. 

    Learn more about our static alert conditions in our [NRQL docs](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#threshold-types).
  </Collapser>
  
  <Collapser
    id="lost-signal"
    title="Lost signal threshold (optional)"
  >
Sometimes an incoming signal can be lost, and it's important to understand if it's simply a delay or an indication of a wider problem. Our **loss signal threshold** lets us know how many seconds the system should wait from the time the last data point was detected before considering the signal to be lost. If the signal does not return before the time limit that you set, you can choose one or both of two actions: open a new incident, and/or close all open incidents. You can close any related open incidents if the lost signal supersedes other incidents on this entity, or if the signal loss was expected.

Setting your lost signal threshold requires knowledge of your system and what you're looking to understand. In the case of `pageviews`, let's say New Relic collects a signal every minute. A lost signal could indicate a much larger latency issue. So, we recommend setting the time to your comfort level and then checking the box to **open a new lost signal incident**.
  </Collapser>
</CollapserGroup>
</Step>

<Step>

## Additional details [#additional-details]

  If the number of  `pageviews` on your `WebPortal` application drops, you'd like to receive a notification as soon as possible. The best quick and efficient action is to create an alert condition that will open an incident if `pageviews` fall below a normal amount.

  This alert condition is a container that holds all the rules—are we using static or anomaly thresholds, are we using a sliding-window aggregation or just leaving the evaluation period as normal? 

  At this point in the process we now have a fully defined container and we've set all the rules to make sure an incident is opened when we want it to be. Based on the settings above, if our alert condition recognizes this behavior in our system that breaches the thresholds that we've set, it will create an incident. Now, all we need to do is to attach this container to a policy. 
  
  The policy is the sorting system for the incident. When you create a policy, you're creating the tool that organizes all of your incoming incidents. You can connect policies to **[workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/)** that tell New Relic where you want all this incoming information to go, how often you want it to be sent, and where. 

<img
    title="name an alert condition "
    alt="A screenshot demonstrating how you can new alert condition "
    src={alertsNameYourAlertCondition}
/>

<CollapserGroup>
  <Collapser
    id="name-your-condition"
    title="Name your condition"
  >
    It's important to give your alert condition a descriptive name. Let's say you name this condition **pageviews** and then you create another condition for a completely different application and label that condition **pageviews** as well. If this occurs you won't be able to distinguish which condition is for which application. So, make sure to give your condition a specific and unique name. In this case, we'd name this condition **pageviews: WebPortal App**.
  </Collapser>

  <Collapser
    id="select-an-existing-policy"
    title="Select an existing policy"
  >

  If you already have a policy you want to connect to an alert condition, then select the existing policy. 

  Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>

  <Collapser
    id="create-a-new-policy"
    title="Create a new policy"
  >
    If we want to create a new policy for this alert condition, here's our chance. Remember, a policy is connected to workflows and workflows control how often we're notified about any incidents. It's a fine balance between ensuring that we learn about any issues with `pagviews` on our `WebPortal` application as quickly as possible and making sure that we don't get so many alerts that our developers experience fatigue and start missing out on important information because of information overload. 

    Policies can hold one or multiple conditions. If you're looking to monitor `pageviews`, you have a few options.
    
    First, you could create a policy that only attaches a single issue per policy (the default option). One issue per policy reduces noise but also requires immediate action. But this means that if you've attached multiple conditions to this policy, not just to **Response time: Example app**, then no matter what, all incidents in this policy will be grouped into one single issue.

    Or we could create one issue per condition. This means that any time the **Response time: Example app** condition opens an incident, all those incidents will be rolled into one issue that is connected to our condition. For this specific use case, you should choose this option because it meets the primary goal, which is to monitor latency issues with `pageviews`. 

    Or we could create an issue for every incident. This option is the most noisy but can work well if you like to send information to an external system. 

    Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>
  <Collapser
    id="close-open-incidents"
    title="Close open incidents"
  >
  An incident will automatically close when the targeted signal returns to a non-breaching state for the time period indicated in the condition's thresholds. This wait time is called the recovery period.

  For example, if the breaching behavior is "`pageviews` are lower than 300 at least once in 5 minutes" then the incident will automatically close when `pageviews` are equal to or higher than 300 for 5 consecutive minutes. 

  When an incident closes automatically:

  1. The closing timestamp is backdated to the start of the recovery period.
  2. The evaluation resets and restarts from when the previous incident ended.

  All conditions have an incident time limit setting that will automatically force-close a long-lasting incident.

  We automatically default to 3 days and recommend that you use our default settings for your first alert. 
  </Collapser>

  <Collapser
    id="custom-incident-description"
    title="Send a custom incident description"
  >
  
  Since we're creating an alert condition that lets us know if there are any latency issues with our `WebPortal` application, we want to make sure our developers have all the information they need when they are notified about this incident. We're going to use workflows to notify a team Slack channel when an incident is created.

  Learn more about custom incident descriptions in our [docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-incident-descriptions).

</Collapser>

  <Collapser
    id="runbook"
    title="Add runbook url"
  >
If you'd like to link to a runbook, you can put the URL in the runbook URL field.
  </Collapser>
</CollapserGroup>

</Step>
</Steps>

## Edit or improve an existing alert condition [#edit-existing-alert-condition]

If you want to edit an alert condition you've already created, you can:

1. Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**.
2. Select **Alert Conditions** in the left navigation.
3. Click on the alert condition you want to edit.

From there, you will be able to see all of the elements you set when creating your condition. You can edit specific elements of the alert condition by clicking the pencil in top right of each section. 

If you have questions about how to edit each section, refer back to `Create a new alert condition` section:

* [Signal coverage](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#create-alert-condition)
* [Signal behavior](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#advanced-signal-settings)
* [Thresholds](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#thresholds)
* [Additional details](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/view-alert-conditions/#additional-details) 

### Signal Coverage [#signal-coverage]

Signal coverage gives a closer look at the most recent signals being ingested by the condition. For Synthetics type conditions, a chart is shown with the 5 most recent `SyntheticChecks`. For all other types, a line chart showing the `NraiSignal` over time for the last few days is shown for easier analysis. This chart is similar to what is shown for an individual issue or incident.

If a condition is disabled, you’ll see an empty state message in this section of the Overview. In the case where the condition is not receiving a signal, you will see an empty state prompting you to adjust your condition settings.

#### Troubleshooting your signal history chart
There are a few reasons that you may see an empty signal history chart.
- The condition is disabled and not producing any signals.
- The condition is a NRQL query and the metric or event it is targeting is not instrumenting data.
- The entity targeted by either the NRQL query or the condition is not instrumenting any data.


## What's next? [#what's-next]
<DocTiles>
  <DocTile title='Get started with New Relic alerts' path="/docs/tutorial-create-alerts/create-new-relic-alerts/"> A beginners guide to alerts</DocTile>
  <DocTile title='Advanced alert conditions' path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/"> Dig deeper into alert conditions</DocTile>
  <DocTile title='Use NRQL conditions' path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/"> Use NRQL to create alert conditions</DocTile>
</DocTiles>




