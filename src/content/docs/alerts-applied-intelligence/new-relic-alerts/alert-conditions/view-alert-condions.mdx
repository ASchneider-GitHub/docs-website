---
title: View an alert condition in the UI
tags:
  - Alerts and applied intelligence
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
freshnessValidatedDate: never
---

New Relic alerts are built with alert conditions. If you want to be notified of any unusual behavior in your stack, you'll first need to create an alert condition. An alert condition is essentially a container that you create to define what conditions need to be met before you're notified of any unusual behavior. There are a few ways of creating alert conditions, but the easiest way is to create an alert condition from a chart and we've provided a step-by-step guide in our [Create your first alert doc](docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition/). You can use our **Alert conditions** page to keep track of all your alert conditions. 

With our alert conditions page you can:
* Browse all of your alert conditions and easily see if there are any open issues
* Open an existing alert conditions to edit thresholds or signal behavior 
* Create a new alert condition 

This doc will walk you through two things: editing and changing an _existing_ alert condition, and creating _brand new alert condition. 

## Create a new alert condition [#create-alert-condition]

The easiest way to get started with alerts is to create an alert from a New Relic chart. This route is the same as creating a NRQL alert condition from scratch, but the chart already has a NRQL query for you to work with. If you're just beginning to use New Relic, we suggest reviewing our [tutorial about creating your first alert](/docs/tutorial-create-alerts/create-an-alert/). 

<Steps>
An alert condition is essentially a container that you create to define what conditions need to be met before you're notified of any unusual behavior. For this example, you're going to create an alert that notifies your team of any latency issues with web transaction time.

So, in this case, if you want to make sure that web transactions never take longer than 50 milliseconds, you'll build an alert condition to monitor when web transaction time goes beyond 50 milliseconds and creates an incident. 

<Step>
<CollapserGroup>
  <Collapser
    id="set-your-query"
    title="Set your query"
  >

Sample query here

</Collapser>
</CollapserGroup>
</Step>

<Step>
## Fine-tune advanced signal settings [#advanced-signal-settings]

New Relic constantly observes the data that streams from your applications and into our system. But not all applications send signals at the same frequency or cadence. Some events could send signals to our system once a minute while others could only report data to New Relic once every day. An alert condition is a **specific** container designed for a **specific** use case. When creating an alert condition this section is the most customizable for the data that you're evaluating. 

We're going to customize these advanced signal settings for our condition that is looking for web transaction latency issues. 

<CollapserGroup>
  <Collapser
    id="window-duration"
    title="Window duration"
  >

  Setting the window duration for your alert condition tells New Relic how to group your data. If you're creating an alert condition for a data set that sends a signal to New Relic once every hour, you'd want to set the window duration to something closer to sixty minutes because it'll help spot patterns and unusual behavior. But, if you're creating an alert condition for web transaction time and New Relic collects a signal for that data every minute, we'd recommend setting the window duration to one minute. 
  
  For your first alert we recommend sticking with our default settings, but the more you get familiar with creating an alert condition we encourage you to customize these fields based on your own experience. 
  </Collapser>

  <Collapser
    id="sliding-window"
    title="Use sliding window aggregation"
  >

  Throughout the day, data streams from your application into New Relic. Instead of evaluating that data immediately for incidents, alert conditions collect the data over a period of time known as the **aggregation window**. An additional delay allows for slower data points to arrive before the window is aggregated.

  Sliding windows are helpful when you need to smooth out "spiky" charts. One common use case is to use sliding windows to smooth line graphs that have a lot of variation over short periods of time in cases where the rolling aggregate is more important than aggregates from narrow windows of time.

  We recommend using our sliding window aggregation if you're not expecting to have a steady and consistent stream of data but are expecting some dips and spikes in data. 

  You can learn more about sliding window aggregation in [this NRQL tutorial](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-sliding-windows/) or by watching this video.

  <Video
    type="youtube"
    id="-5--8DZynFE"
  />

  </Collapser>
  
  <Collapser
    id="streaming-method"
    title="Streaming method"
  >
  In general, we recommend using the **event flow** streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where **event timer** might be a better method to choose, but for your first alert we recommend our default, **event flow**. To better understand which streaming method to choose, we recommend watching this brief video.  

  <Video
  type="wistia"
  id="n6nei987ln"
/>

  </Collapser>

    <Collapser
    id="delay"
    title="Delay"
  >

The delay feature protects you against inconsistent data collection. It gives the alert condition a little wiggle room before deciding to create an incident. If in any given minute your data arrives at New Relic with timestamps from only a single minute, then a low delay setting is enough. On the other hand, if during that minute New Relic receives data points with timestamps from several minutes past or several minutes forward, then your signal is more inconsistent and will need a higher delay setting.

  </Collapser>

  <Collapser
    id="gap-filling-strategy"
    title="Gap-filling strategy"
  >
  We're creating an alert condition to notify our team of any latency issues with web transaction time. In this case, our application consistently sends New Relic data. There is a constant stream of signals being sent from our application to New Relic and there is no expected gap in signal so we won't need to select a gap-filling strategy. For this use case, and for your first alert, we recommend leaving the **gap-filling strategy** set to **none**. 

  If you have a more inconsistent data set sending signals to New Relic once every twenty-four hours, then we'd recommend customizing this feature based on your team's specific needs. 

  Learn more about gap-filling strategies in our [lost signal docs](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).

  </Collapser>
</CollapserGroup>

</Step>

<Step>
### Set thresholds for alert conditions

If an alert condition is a container, then thresholds are the rules that each alert condition contains. As data streams into your system, the alert condition searches for any incidents of these rules. If the alert condition sees data coming in from your system that has met all the conditions you've set, then it will create an incident. An incident is a signal that something is off in your system and you should take a look. 

Your team is creating an alert condition to look for any latency issues in web transaction time. Now, you're going to create the rules this condition is going to look for. 

<CollapserGroup>
  <Collapser
    id="anomaly-threshold"
    title="Anomaly threshold (recommended)"
  >

Your team is creating this alert condition so you'll be notified if web transaction time takes longer than usual. But let's say you don't care how much longer web transaction time is, and you just want to know if transaction time is behaving abnormally. For this specific use case, we'd recommend using our **anomaly threshold**. Our anomaly detection constantly evaluates your data to understand how your system normally behaves. By setting an anomaly threshold, you can use our anomaly detection to alert your team if web transaction time deviates from its expected performance. Since you only want to know if web transaction time is behaving unusually, you'd select **upper and lower** because you want notifications of _all_ deviations. But, if you only want alerts if web transaction times takes _longer_ than usual, you'd select **upper only.**

  Next, you need to set the priority level. The priority level determines what will create an incident. We recommend setting your priority level as **critical** for your first alert. You can learn more about priority levels in our [alert condition docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels).

  Next, you must choose what defines a **critical anomaly threshold breach**. For this first alert, we recommend using our default settings and adjusting to your needs as necessary. So, leave the settings to open an incident "when a query returns a value deviates from the predicted value: **for at least five minutes** by **3 standard deviations**".

  Learn more about anomalies in our anomaly [documentation](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/).

</Collapser>

  <Collapser
    id="static-threshold"
    title="Static threshold"
  >
    Unlike anomaly thresholds, a static threshold doesn't look at your data set as a whole and determines what behavior is unusual based on your system's history. Instead, a static threshold will open an incident whenever your system behaves differently than the criteria that **you set**. Static alert thresholds are much more customizable, and we recommend them if you've a strong sense of your data and what you're looking for. 

    Learn more about our static alert conditions in our [NRQL docs](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#threshold-types).
  </Collapser>
  
  <Collapser
    id="lost-signal"
    title="Lost signal threshold (optional)"
  >
Sometimes an incoming signal can be lost, and it's important to understand if it's simply a delay or an indication of a wider problem. Our **loss signal threshold** lets us know how many seconds the system should wait from the time the last data point was detected before considering the signal to be lost. If the signal does not return before the time limit that you set, you can choose one or both of two actions: open a new incident, and/or close all open incidents. You can close any related open incidents if the lost signal supersedes other incidents on this entity, or if the signal loss was expected.

Setting your lost signal threshold requires knowledge of your system and what you're looking to understand. In the case of web transaction time, let's say New Relic collects a signal every minute. A lost signal could indicate a much larger latency issue. So, we recommend setting the time to your comfort level and then checking the box to **open a new lost signal incident**.
  </Collapser>
</CollapserGroup>
</Step>

<Step>

## Additional details  [#additional details ]

<CollapserGroup>
  <Collapser
    id="name-your-condition"
    title="Name your condition"
  >
    It's important to give your alert condition a descriptive name. Let's say you name this condition **response time** and then you create another condition for a completely different application and label that condition **response time** as well. If this occurs you won't be able to distinguish which condition is for which application. So, make sure to give your condition a specific and unique name. In this case, we'd name this condition **Response time: Example-app**.
  </Collapser>
   </CollapserGroup>

  If there are any latency issues with web transaction times, we'd like to receive a notification as soon as possible. The best quick and efficient action is to create an alert condition that will open an incident if web transaction times take too long.
 
  This alert condition is a container that holds all the rules—are we using static or anomaly thresholds, are we using a sliding-window aggregation or just leaving the evaluation period as normal? 

  At this point in the process we now have a fully defined container and we've set all the rules to make sure an incident is opened when we want it to be. Based on the settings above, if our alert condition recognizes this behavior in our system that breaches the thresholds that we've set, it will create an incident. Now, all we need to do is to attach this container to a policy. 
  
  The policy is the sorting system for the incident. When you create a policy, you're creating the tool that organizes all of your incoming incidents. You can connect policies to **[workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/)** that tell New Relic where you want all this incoming information to go, how often you want it to be sent, and where. 

<CollapserGroup>
  <Collapser
    id="select-an-existing-policy"
    title="Select an existing policy"
  >

  If you already have a policy you want to connect to an alert condition, then select the existing policy. 

  Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>

  <Collapser
    id="create-a-new-policy"
    title="Create a new policy"
  >
    If we want to create a new policy for this alert condition, here's our chance. Remember, a policy is connected to workflows and workflows control how often we're notified about any incidents. It's a fine balance between ensuring that we learn about any issues with web transaction time as quickly as possible and making sure that we don't get so many alerts that our developers experience fatigue and start missing out on important information because of information overload. 

    Policies can hold one or multiple conditions. If you're looking to monitor web transaction latency, you have a few options.
    
    First, you could create a policy that only attaches a single issue per policy (the default option). One issue per policy reduces noise but also requires immediate action. But this means that if you've attached multiple conditions to this policy, not just to **Response time: Example app**, then no matter what, all incidents in this policy will be grouped into one single issue.

    Or we could create one issue per condition. This means that any time the **Response time: Example app** condition opens an incident, all those incidents will be rolled into one issue that is connected to our condition. For this specific use case, you should choose this option because it meets the primary goal, which is to monitor latency issues with web transaction time. 

    Or we could create an issue for every incident. This option is the most noisy but can work well if you like to send information to an external system. 

    Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>
</CollapserGroup>
</Step>

<Step>
## Additional settings [#additional-settings]

<CollapserGroup>
  <Collapser
    id="close-open-incidents"
    title="Close open incidents"
  >
  An incident will automatically close when the targeted signal returns to a non-breaching state for the time period indicated in the condition's thresholds. This wait time is called the recovery period.

  For example, if the breaching behavior is "web transaction time is longer than .50 seconds at least once in 5 minutes," then the incident will automatically close when web transaction time is equal to or lower than .50 for 5 consecutive minutes. 

  When an incident closes automatically:

  1. The closing timestamp is backdated to the start of the recovery period.
  2. The evaluation resets and restarts from when the previous incident ended.

  All conditions have an incident time limit setting that will automatically force-close a long-lasting incident.

  We automatically default to 3 days and recommend that you use our default settings for your first alert. 
  </Collapser>

  <Collapser
    id="custom-incident-description"
    title="Send a custom incident description"
  >
  
  Since we're creating an alert condition that lets us know if there are any latency issues with web transaction time, we want to make sure our developers have all the information they need when they are notified about this incident. We're going to use workflows to notify a team Slack channel when an incident is created.

  Learn more about custom incident descriptions in our [docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-incident-descriptions).

</Collapser>

  <Collapser
    id="runbook"
    title="Add runbook url"
  >
If you'd like to link to a runbook, you can put the URL in the runbook URL field.
  </Collapser>
</CollapserGroup>

</Step>
</Steps>

## Edit or improve an existing alert condition 
If you want to edit an alert condition you've already created, you can:

1. Go to **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities)** > **Alerts & AI** .
2. Select ** Alert Conditions** in the left navigation.
3. Click on the alert condition you want to edit. 

From there, you will be able to see all of the elements you set when creating your condition. You can edit specific elements of the alert condition by clicking the pencil in top right of each section.

### Signal Coverage [#signal-coverage]

Signal Coverage gives a closer look at the most recent signals being ingested by the condition. For Synthetics type conditions, a chart is shown with the 5 most recent SyntheticChecks. For all other types, a line chart showing the NraiSignal over time for the last few days is shown for easier analysis. This chart is similar to what is shown for an individual Issue or Incident.

If a condition is disabled, you’ll see an empty state message in this section of the Overview. In the case where the condition is not receiving a signal, you will see an error message prompting you to adjust your condition settings.

### Signal Behavior [#signal-behavior]
Window duration
1 minute
Sliding window aggregation
Disabled
Slide by interval
Not set
Streaming method
Event flow
Delay
2 minute

### Thresholds [#thresholds]
This section shows you all the thresholds you set while creating your alert condition. 

### Additional details
This section gives the rest of the details added during initial configuration of the Condition, as well as a link to the associated Policy.

## What's next


