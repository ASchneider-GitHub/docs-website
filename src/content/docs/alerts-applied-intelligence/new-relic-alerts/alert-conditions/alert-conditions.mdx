---
title: Alert conditions
tags:
  - Alerts and applied intelligence
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition/
  - /docs/alerts/alert-policies/configuring-alerts/managing-your-alerts 
freshnessValidatedDate: never
---
import alertsSetaQuery from 'images/alerts_screenshot-crop_set-a-query.webp'

import alertsFineTuneAlertSignals from 'images/alerts_screenshot-crop_fine-tune-alert-signals.webp'

import alertsSetaThresholdforanAlertCondition from 'images/alerts_screenshot-crop_set-a-threshold-for-an-alert-condition.webp'

import alertsNameYourAlertCondition from 'images/alerts_screenshot-crop_name-your-alert-condition.webp'

import alertsAlertConditionPage from 'images/alerts_screenshot-crop_alert-condition-page.webp'

An alert condition is the core element that defines when an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/incident-event-attributes/#definition) is created. It acts as the essential starting point for building any meaningful alert. Alert conditions contain the parameters or thresholds that need to be met before you're informed.  They can mitigate excessive alerting, or let your team know when new or unusual behavior appears.

<img
    title="alert condition homepage"
    alt="A screenshot showing the alert condition homepage"
    src={alertsAlertConditionPage}
/>
<figcaption>
The **Alert conditions list** page is the universal hub for all of your alert conditions. 
</figcaption>

## Create a new alert condition [#create-alert-condition]

An alert condition is a continuously running query that measures a given set of events against a defined threshold, and opens an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) when the threshold is met for a defined window of time. 

This example shows you how to manually create a new alert condition using the **Alert condition details** page. But there are a lot of ways to create an alert condition. You can create an alert condition from:

* [A chart](/docs/tutorial-create-alerts/create-an-alert/)
* [A policy page](https://one.newrelic.com/nr1-core/condition-builder/policy-entity)
* [The **Alert coverage gaps** page](https://one.newrelic.com/alerts-ai/detection-gaps/)

You can also use one of our alert builders:
* Use **Write your own query** to build alerts from scratch
* **Use guided mode** 

For all methods except for our guided mode, the process for creating an alert condition will be _exactly_ the same as described in the steps below. 

<Steps>
<Step>
### Set your signal behavior
In this example, let's imagine that your team manages the `WebPortal` application for an e-commerce site. You want be alerted of any latency issues. 

To create a new alert condition:

* Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**
* Select **Alert Conditions** in the left navigation.
* Then select **New alert condition**.
* Select **Write your own query**. 

<CollapserGroup>
  <Collapser
    id="set-your-query"
    title="Set your signal behavior"
  >

You can use a NRQL query to define the signals that you want an alert condition to use as the foundation for your alert. For this example, you will be using this query:

```
SELECT average(duration) 
FROM PageView 
WHERE appName = 'WebPortal'
```
Using this query for your alert condition tells New Relic that you want to know the average `pageviews` from your `WebPortal` application. Monitoring `pageviews` can help you look out for any latency issues in your application. 

You can learn more about how to use NRQL, New Relic's query language, see our [NRQL documentation](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/). 
</Collapser>
</CollapserGroup>

</Step>

<Step>
### Fine-tune advanced signal settings [#advanced-signal-settings]

After you've defined your signal, click **Run**. A chart will appear and display the parameters that you've set. 

<img
    title="set a query"
    alt="A screenshot showing a user how to set the signal behavior for an alert condition"
    src={alertsSetaQuery}
/>

For this example, the chart with show the average `pageviews` for your `WebPortal` application. Click **Next** and begin configuring your alert condition. 

For this example, you're going to customize these advanced signal settings for the condition you created to monitor an unusual activity for `pageviews` in your `WebPortal` application. 

<CollapserGroup>
  <Collapser
    id="window-duration"
    title="Window duration"
  >

<img
    title="fine tune alert settings"
    alt="A screenshot depicting advanced signal settings"
    src={alertsFineTuneAlertSignals}
/>
The window duration defines how New Relic groups your data for analysis in an alert condition. Choosing the right setting depends on your data's frequency and your desired level of detail:

* **High-frequency data (for example, pageviews every minute)**: Set the window duration to match the data frequency (1 minute in this case) for real-time insights into fluctuations and trends.
* **Low-frequency data (for example, hourly signals)**: Choose a window duration that captures enough data to reveal patterns and anomalies (for example, 60 minutes for hourly signals).

Remember, you can always customize the window duration based on your specific needs and experience. We recommend using the defaults when starting out and experimenting as you become more comfortable with creating alert conditions.
  </Collapser>

  <Collapser
    id="sliding-window"
    title="Use sliding window aggregation"
  >

  When dealing with data that's sparsely populated or exhibits significant fluctuations between intervals, traditional aggregation methods can fall short. Here's how to use sliding window aggregation to effectively analyze such data and trigger timely alerts:

  1. **Smooth out the noise**: Start by creating a large aggregation window. This window (for example, 5 minutes) acts as a buffer, smoothing out the inherent "noise" or variability in your data. This helps prevent spurious alerts triggered by isolated spikes or dips.
  2. **Avoid lag with a sliding window**: While a large window helps in data analysis, if you wait for the entire interval to elapse before checking thresholds, you can experience significant delays in alert notifications. We recommend smaller sliding windows (for example, one minute). Imagine this sliding window as a moving frame scanning your data within the larger aggregation window. Each time the frame advances by its smaller interval, it calculates an aggregate value (for example, average).
  3. **Set your threshold duration**: Now, you can define your alert threshold within the context of the smaller sliding window. This allows you to trigger alerts quickly when the aggregate value in the current frame deviates significantly from the desired range, without sacrificing the smoothing effect of the larger window.

  You can learn more about sliding window aggregation in [this NRQL tutorial](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-with-sliding-windows/).
  </Collapser>
  
  <Collapser
    id="streaming-method"
    title="Streaming method"
  >
  In general, we recommend using the **event flow** streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where **event timer** might be a better method to choose, but for your first alert we recommend our default, **event flow**. To better understand which streaming method to choose, we recommend watching this brief video.  

  <Video
  type="wistia"
  id="n6nei987ln"
/>

  </Collapser>

    <Collapser
    id="delay"
    title="Delay"
  >

  The delay feature in alert conditions serves as a safeguard against potential issues arising from inconsistent data collection. It acts as a buffer, allowing extra time for data to arrive and be processed before an alert is triggered. This helps prevent false positives and ensures more accurate incident creation.

  How it works:

  The appropriate delay setting is determined by evaluating the consistency of your incoming data:
    * Consistent data: If data points consistently arrive with timestamps within a single minute, a lower delay setting is sufficient.
    * Inconsistent data: If data points arrive with timestamps spanning multiple minutes in the past or future, a higher delay setting is necessary to accommodate the inconsistency.
Creating a buffer:
    * The selected delay setting introduces a waiting period before the alert condition assesses data against defined thresholds.
    * This buffer allows time for data discrepancies to settle, reducing the likelihood of misleading alerts.

  </Collapser>

  <Collapser
    id="gap-filling-strategy"
    title="Gap-filling strategy"
  >
  You're creating an alert condition to notify your team of any latency issues with the `WebPortal` application. In this example, your application consistently sends New Relic data. There is a constant stream of signals being sent from your application to New Relic and there is no expected gap in signal so you won't need to select a gap filling strategy. 

  Gap filling strategies are designed to address scenarios where data collection might be intermittent or incomplete. They provide a method for substituting missing data points with estimated values, ensuring that alert conditions can still function effectively even with gaps in the data stream.

  When to Leave Gap Filling Off:
    * **Consistent Data Flow**: If your application consistently sends data to New Relic without expected gaps, as in the case of the WebPortal application, gap filling is generally not necessary. Leaving the gap-filling strategy set to none is often the most appropriate approach in such cases.
  Key Considerations:
    * **Popular Use Case**: A common use of gap filling is to insert a value of 0 for windows with no data received.
    * **Anomaly Thresholds**: When using anomaly thresholds, the gap-filling value is interpreted as the number of standard deviations from the last observed value. For example, filling gaps with a value of 0 would replicate the last value seen, effectively assuming no change.
  
Learn more about gap-filling strategies in our [lost signal docs](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).

  </Collapser>
</CollapserGroup>

</Step>

<Step>
### Set thresholds for alert conditions [#thresholds]

If an alert condition is a container, then thresholds are the rules that each alert condition must follow. As data streams into your system, the alert condition searches for any incidents of these rules. If the alert condition sees data coming in from your system that has met all the conditions you've set, then it will create an incident. An incident is a signal that something is off in your system and you should take a look. 

  <img
    title="set a threshold"
    alt="A screenshot depicting how to set threshold for an alert condition "
    src={alertsSetaThresholdforanAlertCondition}
/>

<CollapserGroup>
  <Collapser
    id="anomaly-threshold"
    title="Anomaly threshold"
  >

Anomaly thresholds are ideal when you're more concerned about deviations from expected patterns than specific numerical values. They enable you to monitor for unusual activity without needing to set predefined limits. New Relic's anomaly detection dynamically analyzes your data over time, adapting thresholds to reflect evolving system behavior.
Setting Up Anomaly Detection:
  1. Choose Upper or Lower:
      * Select upper and lower to be alerted about any deviations, both higher and lower than expected.
      * Select upper only to focus solely on unusually high values.
  2. Assign Priority:
      * Set the priority level to critical for your initial alert to ensure prompt attention to potential issues.
      * Refer to the alert condition docs for more details on priority levels.
  3. Define Breach Criteria:
      * Start with the default settings: open an incident when a query returns a value that deviates from the predicted value for at least five minutes by 3 standard deviations.
      * Customize these settings as needed to align with your specific application and alerting requirements.

  You can learn more about priority levels in our [alert condition docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels).
  Learn more about anomaly threshold and model behaviors in our anomaly [documentation](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/).

</Collapser>

  <Collapser
    id="static-threshold"
    title="Static threshold"
  >

    Unlike anomaly thresholds, a static threshold doesn't look at your data set as a whole and determines what behavior is unusual based on your system's history. Instead, a static threshold will open an incident whenever your system behaves differently than the criteria that **you set**. 

    For both anomaly and static thresholds you need to set the priority level.  See the section above for more details.  
  </Collapser>
  
  <Collapser
    id="lost-signal"
    title="Lost signal threshold (optional)"
  >

  The loss signal threshold determines how long to wait before considering a missing signal lost. If the signal doesn't return within that time, you can choose to open a new incident or close any related open ones. Set the threshold based on your system's expected behavior and data collection frequency. For example, if a lost signal for `pageviews` could indicate a latency issue, set a threshold you're comfortable with and check the box to open a new lost signal incident.

  </Collapser>
</CollapserGroup>
</Step>

<Step>

### Add alert condition details [#add-details]

  At this point in the process you now have a fully defined condition and you've set all the rules to make sure an incident is opened when you want it to be. Based on the settings above, if your alert condition recognizes this behavior in your system that breaches the thresholds that you've set, it will create an incident. Now, all you need to do is to name this condition and attach it to a policy. 
  
  The policy is the sorting system for the incident. When you create a policy, you're creating the tool that organizes all of your incoming incidents. You can connect policies to **[workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/)** that tell New Relic where you want all this incoming information to go, how often you want it to be sent, and where. 

  <img
    title="name an alert condition "
    alt="A screenshot demonstrating how you can new alert condition "
    src={alertsNameYourAlertCondition}
/>

<CollapserGroup>
  <Collapser
    id="name-your-condition"
    title="Name your condition"
  >

    It's important to give your alert condition a descriptive name. Let's say you name this condition **pageviews** and then you create another condition for a completely different application and label that condition **pageviews** as well. If this occurs you won't be able to distinguish which condition is for which application. So, make sure to give your condition a specific and unique name. In this case, you''d name this condition **pageviews: WebPortal App**.
  </Collapser>

  <Collapser
    id="select-an-existing-policy"
    title="Select an existing policy"
  >

  If you already have a policy you want to connect to an alert condition, then select the existing policy. 

  Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>

  <Collapser
    id="create-a-new-policy"
    title="Create a new policy"
  >
  Balancing responsiveness and fatigue in your alerting strategy is crucial, and you've laid out the key considerations regarding `pageview` monitoring for your `WebPortal` application. Let's explore the policy options:

  1. One issue per policy (default):
      * Pros: Reduces noise and ensures immediate action.
      * Cons: Groups all incidents under one issue, even if triggered by different conditions. Not ideal for multiple pageview concerns.
  2. One issue per condition:
      * Pros: Creates separate issues for each condition, ideal for isolating and addressing specific latency issues.
      * Cons: Can generate more alerts, potentially leading to fatigue.
3. An issue for every incident:
      * Pros: Provides granular detail for external systems, but not optimal for internal consumption due to potential overload.
      * Cons: Most noisy option, challenging to track broader trends and prioritize effectively.

    Learn more about how to create policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
  </Collapser>
  <Collapser
    id="close-open-incidents"
    title="Close open incidents"
  >
  An incident will automatically close when the targeted signal returns to a non-breaching state for the time period indicated in the condition's thresholds. This wait time is called the recovery period.

  For example, if the breaching behavior is "`pageviews` are lower than 300 at least once in 5 minutes" then the incident will automatically close when `pageviews` are equal to or higher than 300 for 5 consecutive minutes. 

  When an incident closes automatically:
  1. The closing timestamp is backdated to the start of the recovery period.
  2. The evaluation resets and restarts from when the previous incident ended.

  All conditions have an incident time limit setting that will automatically force-close a long-lasting incident.

  New Relic automatically defaults to 3 days and recommends that you use our default settings for your first alert. 

  Another way to close an open incident when the signal does not return any data, is by configuring a `loss of signal` threshold. Refer to the lost signal threshold section above for more details. 
  </Collapser>

  <Collapser
    id="custom-incident-description"
    title="Send a custom incident description"
  >
  
  Since you're creating an alert condition that lets you know if there are any latency issues with your `WebPortal` application, you want to make sure your developers have all the information they need when they are notified about this incident. You're going to use workflows to notify a team Slack channel when an incident is created.

  Learn more about custom incident descriptions in our [docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-incident-descriptions).

</Collapser>

  <Collapser
    id="runbook"
    title="Add runbook url"
  >
If you'd like to link to a runbook, you can put the URL in the runbook URL field.
  </Collapser>
</CollapserGroup>
</Step>
</Steps>

## Edit or improve an existing alert condition [#edit-existing-alert-condition]

If you want to edit an alert condition you've already created, you can:

1. Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > **Alerts & AI**.
2. Select **Alert Conditions** in the left navigation.
3. Click on the alert condition you want to edit

From there, you will see the **Alert conditions details** page. This page contains all the elements you set when you created your condition. You can edit specific elements of the alert condition by clicking the pencil in top right of each section. 

### Signal history [#signal-coverage]

Under **Signal history** you can see the most recent results for the NRQL query you used to create your alert condition. For this example, you would see the average `pageviews` on the `WebPortal` app for the specific time frame you've set. 

For all alert conditions built with NRQL queries, the **Signal history** will be presented with a line chart. 

Any alert condition built with a synthetic monitor will be a bit different. This is because synthetic monitors allow you to ping your application from multiple locations which can produce positive or negative results each time the monitor runs. This data can only be presented with a table. 

## Troubleshoot 
If you see an empty signal in the history chart, consider one of the following: 

* **Review the condition's settings**: Double-check that all elements are configured correctly.
* **Inspect NRQL queries**: Ensure they're targeting valid metrics or events and returning data.
* **Examine entity configuration**: Confirm that the entity is set up properly to send signals.
* **Consult New Relic documentation**: Refer to relevant guides for assistance with specific issues.

## What's next? [#whats-next]
<DocTiles>
  <DocTile title='Create your first New Relic alert' path="/docs/tutorial-create-alerts/create-new-relic-alerts/" label={{text: "Start here", color: "orange"}}>A crash course in alerts for beginners </DocTile>
  <DocTile title='Advanced alert conditions' path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/"> If you've already set up your alert conditions, dig deeper with advanced settings</DocTile>
  <DocTile title='Get notified' path="/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/"> Use workflows to get notified about any unusual behavior in your stack</DocTile>
</DocTiles>




