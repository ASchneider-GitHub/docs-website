---
title: 'Explore AI data in New Relic'
metaDescription: 'AI Monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import apmAiResponsesDefaultView from 'images/apm_screenshot-crop_ai-responses-default-view.webp'

import apmAIEntitiesPage from 'images/apm_screenshot-crop_AI-entities-page.webp'

import apmFindAiResponsesFromApm from 'images/apm_screenshot-crop_find-ai-responses-from-apm.webp'

Now that you've enabled AI monitoring, New Relic can capture performance metrics and trace data about your AI app. AI monitoring lets you evaluate how well your app performans from an end-user point of view and how much it costs to deliver that end user experience. 

<img
    title="AI Entities page"
    alt="A screenshot of the first page you see when you click AI Monitoring. View aggregated data, compare your AI models, or create drop filters."
    src={apmAIEntitiesPage}
/>
<figcaption>
    Start exploring your AI data from the AI entities page: Go to **[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI Monitoring**, then choose one of our entry points to explore your data.
</figcaption>

From **[one.newrelic.com](https://one.newrelic.com)**, go to **All Capabilities** And choose **AI Monitoring**. The AI entities page gives you three entry points into managing your data:

* **AI Responses**: Overview aggregated data about all your AI entities. 
* **Compare Models**: Compare token usage, response time, and error rate between different models. If you're conducting A/B tests, get all the information you need to make decisions about your AI-powered app.
* **Drop filters**: Build queries that direct AI monitoring to drop sensitive information before it's stored in New Relic.

## Scope your data by AI entity [#track-ai-metrics]

Scoping your data by AI entity takes you to the APM summary page, where you can view data about your app. To find data about your app's AI-layer, choose **AI Responses**: 

<img
    title="Find entity-scoped AI Responses page"
    alt="A screenshot of the APM summary page. A green arrow highlights where to find data about the AI-layer of your app"
    src={apmFindAiResponsesFromApm}
/>
<figcaption>
    To find data about the AI-layer of your app, click **AI Responses** from the APM summary page. 
</figcaption>

Similar to the AI responses page that aggregates all your AI data together, the entity-scoped AI responses page displays data about token usage, total responses, and how these metrics trend over time. 

<img
    title="AI monitoring response page"
    alt="A screenshot that shows the default view for AI responses "
    src={apmAiResponsesDefaultView}
    />

<figcaption>
    To access your AI data: Go to **[one.newrelic.com](https://one.newrelic.com) > All capabilities > AI Monitoring**, then choose your AI entity.
</figcaption>

This page lets you: 

* **Query your responses with the filter bar.** The filter bar lets you search the content of your AI's responses, so you can correlate AI behavior with token usage or response time. For example, if you notice that a particular subject requires more tokens than usual to generate an accurate response, you might 
* ** Pinpoint when a change occurs with the time series graphs**. Click the dropdown to toggle between token usage, total responses, and average response time.
* **Overview user prompts and AI responses with the responses table**. 


## Evaluate AI responses with the Responses table  [#responses-table]

Click a response from the table to open the trace view. This gives you specific information about a particular response. 

```
    <img
        title="Trace view for an individual AI response"
        alt="The trace view "
        src={aiTracePage}
    />

    <figcaption>See how your AI interacts with other AI models, API calls, database lookups, and custom application code. Click **Trace** to view trace data, or **Logs** to view your logs.</figcaption>
```


## ELEPHANT responses with traces [#filter]

You can filter your data by attributes or keywords to expose some responses from your total responses. When you add a query, your tiles, time series charts, and table will update to reflect only that data. 

Following the screenshot above, you can use this page to:
1. Overview request duration and total tokens. For example, in this screenshot, you can see that the response had high latency at 21.7 seconds. 
2. Look at summary metrics for all sub-processes called when constructing a response. You can use this section to see how an operation contributed to a higher overall duration. 
3. Follow an end user's request until it reaches completion. The colored bars above a trace indicates the relative duration of every step. Click the spans to view span details. 
4. See additional context about your AI's response. If you're concerned about total tokens, see the ratio between request and completion tokens. If you have multiple models in your toolchain, check which model generated a particular response. 

```
    <img
        title="AI Response filter bar with query"
        alt="The filter bar lets you expose the data you need to troubleshoot a  problem with your AI stack."
        src={aiFilterBar}
    />
    <figcaption>**AI Response > filter bar**: Query your data with various attributes to drill down into specific responses.</figcaption>
```

Because AI Monitoring lets you view all of your AI's responses, you may want to isolate certain kinds of data. You can query by attributes, which you can find by clicking the `+` next to the bar. You can also search your requests and responses by keyword. Some examples are:

* **Testing different models**: If you're testing different models, filter by model: `Response model IN anthropic.claude-instant-v1`. This query shows only data related to that particular AI model.
* **Query for keywords**: If you want to view responses or requests that ask about pricing, you can update the query to read `Request LIKE cheapest` or `Response LIKE expensive`. This exposes inputs and outputs where those keywords appear.

## Compare how your models perform [#model-comparison-view]

## What's next? [whats-next]

 