---
title: "Control your data with NRQL"
tags:
  - NRQL tutorial
  - 'NRQL: New Relic query language'
  - Get started
  - Tutorial
metaDescription: "Learn how to use NRQL to control your data."
---

import queriesnrql3tutorial1 from 'images/queries-nrql_screenshot-crop-tutorial3-1.webp'

import queriesnrql3tutorial2 from 'images/queries-nrql_screenshot-crop-tutorial3-2.webp'

import queriesnrql3tutorial3 from 'images/queries-nrql_screenshot-crop-tutorial3-3.webp'

import queriesnrql3tutorial4 from 'images/queries-nrql_screenshot-crop-tutorial3-4.webp'

import queriesnrql3tutorial5 from 'images/queries-nrql_screenshot-crop-tutorial3-5.webp'

import queriesnrql3tutorial6 from 'images/queries-nrql_screenshot-crop-tutorial3-6.webp'

import queriesnrql3tutorial7 from 'images/queries-nrql_screenshot-crop-tutorial3-7.webp'

import queriesnrql3tutorial8 from 'images/queries-nrql_screenshot-crop-tutorial3-8.webp'

import queriesnrql3tutorial9 from 'images/queries-nrql_screenshot-crop-tutorial3-9.webp'

import queriesnrql3tutorial10 from 'images/queries-nrql_screenshot-crop-tutorial3-10.webp'

import queriesnrql3tutorial11 from 'images/queries-nrql_screenshot-crop-tutorial3-11.webp'

import queriesnrql3tutorial12 from 'images/queries-nrql_screenshot-crop-tutorial3-12.webp'

import queriesnrql3tutorial13 from 'images/queries-nrql_screenshot-crop-tutorial3-13.webp'

import queriesnrql3tutorial14 from 'images/queries-nrql_screenshot-crop-tutorial3-14.webp'

import queriesnrql3tutorial15 from 'images/queries-nrql_screenshot-crop-tutorial3-15.webp'

import queriesnrql3tutorial16 from 'images/queries-nrql_screenshot-crop-tutorial3-16.webp'

import queriesnrql3tutorial17 from 'images/queries-nrql_screenshot-crop-tutorial3-17.webp'

import queriesnrql3tutorial18 from 'images/queries-nrql_screenshot-crop-tutorial3-18.webp'

import queriesnrql3tutorial19 from 'images/queries-nrql_screenshot-crop-tutorial3-19.webp'

Welcome to Level 2. In the previous level, we explored the fundamentals of building queries and manipulating results to retrieve the data we want. We covered basic query structure, defining time windows, and how to select specific attributes to observe. We also began learning how to aggregate and display data on line graphs and other visualizations.

In this section, we will delve deeper and explore even more functions for creating interesting aggregations. We will learn how to compare returned data with previous time periods, group data into more specific time windows, be more granular with `SINCE` and `UNTIL` functionality, and explore using wildcards in filters. We will even discuss how to rename attributes to be more user-friendly.

We had a gentle introduction to aggregation in Level 1 by using count(), average(), sum(), max() and min() to transform data in meaningful ways. Next we will explore even more powerful functionality. In this lesson we will learn how to find and count unique values, locate the most recent or oldest entries for an attribute, and work with percentages and percentiles.

You'll learn how to:

* Query for unique values using uniques(attributeName)
* Determine how many unique values exist in an attribute using uniqueCount(attributeName)
* Retrieve the earliest(attributeName) and latest(attributeName) within a specific time window
* Calculate percentages based on a qualifer or other data point with percentile()
* Perform basic math using attributes and aggregation functions, or a combination of both
* Cast attribute names to something custom and more readable
* Search to include/exclude using wildcards with LIKE/NOT LIKE, or limit results to those IN a list or NOT IN that list
* Query within more advanced time windows using dates, epoch, and WITH TIMEZONE
* Group data into interesting time windows using time-based cohorting

Again if you have chosen an account without Transaction event reporting, we will fallback to other queries that are similar. Let's get started!

### Aggregate functions
<Steps>
    <Step>
In a previous lesson we learned how the count() function can return a simple count of available records. To determine the number of unique values recorded for an attribute over a specified time range, we can use the uniqueCount() function. In this function, we provide the attribute we want to inspect as an argument. For instance, here we display all the unique hosts:

<SideBySide>
    <Side>
```sql
SELECT uniqueCount(http.url) 
FROM Public_APICall 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Aggregate functions"
    alt="A screenshot displaying a query for aggregating functions"
    src={queriesnrql3tutorial1}
/>
    </Side>
</SideBySide>

To optimize query performance, the function above returns approximate results for queries that inspect more than 256 unique values. To return the actual list of unique values for an attribute over a specified time range, we can use the uniques() function. 

```sql
SELECT uniques(http.url) 
FROM Public_APICall 
SINCE 1 day ago
```

A second limit parameter may be provided: uniques(attribute[,limit]). When it is not provided, the default limit of 1,000 unique attribute values per facet is applied. You may specify a different limit value, up to a maximum of 10,000.
    </Step>
    <Step>
To retrieve the most recent value of an attribute over a specified time range, use the latest() function. In this sample query, we are locating the most recent response time for a web transaction in the last day. This could help us locate the latest value for an intermittently reporting transaction or service.

<SideBySide>
    <Side>
```sql
SELECT latest(duration) 
FROM Public_APICall 
WHERE awsAPI = 'sqs' 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Aggregate with the latest function"
    alt="A screenshot displaying a query for aggregating using the latest function"
    src={queriesnrql3tutorial2}
/>
    </Side>
</SideBySide>

    </Step>
    <Step>
Using the earliest() function will do the opposite; that is, it will return the earliest value of an attribute recorded in the specified time range. In this sample query, we retrieve the earliest response time for a web transaction in the last day. If data is consistently reporting, this will simply be the data point from the earliest event 24 hours ago.

<SideBySide>
    <Side>
```sql
SELECT earliest(duration) 
FROM Public_APICall 
WHERE awsAPI = 'sqs' 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Aggregate with the earliest function"
    alt="A screenshot displaying a query for aggregating using the earliest function"
    src={queriesnrql3tutorial3}
/>
    </Side>
</SideBySide>
    </Step>
    <Step>
There may also be scenarios in which you need percentages instead of counts, sums, or averages. Using the percentage() function allows you to calculate the percentage of a value in the data set that matches a specified condition. This function takes two arguments: the first is an aggregator function for your desired attribute, such as count(). The second is a WHERE condition to specify the subset of data you'd like to query.

In this sample query, we are finding the percentage of transactions over the last day that had a duration (or response time) greater than 100 milliseconds.

<SideBySide>
    <Side>
```sql
SELECT percentage(count(*), WHERE duration > 0.1) 
FROM Public_APICall 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Aggregate with the where function"
    alt="A screenshot displaying a query for aggregating using the where condition"
    src={queriesnrql3tutorial4}
/>
    </Side>
</SideBySide>
    </Step>
    <Step>
It's very common to view application performance and/or customer experience data using percentiles rather than averages. With the percentile() function we can understand the experience of the nth percentile.

For example, let's say we want to know what the worst experience (highest duration) of 98% of our customers is today. We can ask NRDB for percentile(duration, 98) from the last 24 hours.

<SideBySide>
    <Side>
```sql
SELECT percentile(duration,98) 
FROM Public_APICall 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Aggregate with percentile duration"
    alt="A screenshot displaying a query for aggregating using the percentile duration function"
    src={queriesnrql3tutorial5}
/>
    </Side>
</SideBySide>
    </Step>
</Steps>

As you can see, aggregation can manipulate data in powerful ways. We used uniqueCount() to count the unique entries of a particular attribute. But you could also use this to identify a count of unique machines, reporting containers, or even how many custom data points are sent to New Relic. And, if we want to know what unique values are available to query, we can always ask for a list using uniques().

The latest()/earliest() functions are particularly useful when dealing with sparse data, or investigating when something began or stopped reporting (assuming the data is still stored in New Relic).

We also learned that percentage() can determine what percentage of events matched a qualifier compared with the overall result set.

Finally, we saw how to observe percentiles. For example, you can use percentile() as a Key Performance Indictator by setting a goal to ensure 90% of all end user transactions are faster than a provided duration.

We are getting much more advanced at dancing with our data! In the next lesson we will learn how to do basic mathematics with NRQL.

### Use math operators

<Steps>
    <Step>
NRQL supports basic math operators. You can perform addition (+), subtraction (-), multiplication (*), and division (/) on both numeric attribute values, and results of aggregator functions.

For example, when a transaction event is recorded we capture both the total response time (as duration) and database response time (as databaseDuration). But what if we want to compute the average time spent outside of database processing? Well, let's start by calcuating that value for each event in our data set.

```sql
SELECT GigabytesIngestedBillable - GigabytesIngestedFree, metric  
FROM NrMTDConsumption 
WHERE productLine IS NOT NULL
```

    </Step>
    <Step>
Great! We just performed some basic math. This is useful if we are listing individual events. But what if we want to know the average duration of transactions without the database time? Conveniently, we can do the arithmetic within the function:

```sql
SELECT average(GigabytesIngestedBillable - GigabytesIngestedFree) 
FROM NrMTDConsumption 
WHERE GigabytesIngestedBillable IS NOT NULL
``` 
    </Step>
    <Step>
Now, what if we wanted to get even more complicated and subtract, divide, and multiply in the same query to figure out the duration without database time, as a percentage of overall time? Well, we can add the math:

```sql
SELECT average(GigabytesIngestedBillable - GigabytesIngestedFree) / unitPrice * 100 
FROM NrMTDConsumption 
WHERE GigabytesIngestedBillable IS NOT NULL
```
    </Step>
</Steps>

New Relic reports timings as part of your events, so you can use them to calculate interesting data points, or even generate percentage results.

You can further maximize the power of basic mathematics by doing things like timing custom actions or events, or sending custom data. For instance, an e-commerce platform that reports data about order sizes, which transactions were successful orders, and payment methods could calculate values such as the conversion rate of orders vs. unique customer visits.

### Label attributes

<Steps>
    <Step>
As you start performing more complex NRQL functions, you will notice that the name displayed for queried attributes might not be helpful - especially for others that don't know NRQL! Let's consider an example using what we learned in basic math.

```sql
SELECT average(duration-externalDuration) 
FROM Transaction
```

    </Step>
    <Step>
We can use the AS clause after a function or attribute to give the result a more readable, meaningful name. This makes it easier for you and your team(s) to understand exactly what a chart represents.

```sql
SELECT average(duration-externalDuration) 
AS 'Non-External Response Time' 
FROM Transaction
```
    </Step>
This may seem purely aesthetic, but when you're building detailed dashboards, it's important to clearly label your data. This ensures zero ambiguity for anyone viewing your widgets, billboards, line charts or tables.

We'll refer back to this in upcoming examples about grouping, to explore how AS can create clean result sets in more advanced scenarios too.
</Steps>

### Compare time windows

<Steps>
    <Step>
We have learned how to use time ranges with SINCE and UNTIL clauses. But what if we want to compare values fromdifferent time ranges? We can achieve this with the COMPARE WITH clause.

We use SINCE and UNTIL to define our period of interest. Then, we denote the time period we'd like to compare against with a COMPARE WITH [time period] AGO clause containing a relative offset value.

Specifically, in the sample query below we compare the last day against the previous week using a relative offset of 1 week ago.   

<SideBySide>
    <Side>
```sql
SELECT average(duration) 
FROM Public_APICall 
SINCE 1 day ago 
COMPARE WITH 1 week ago
```
    </Side>
    <Side>
<img
    title="Compare time windows"
    alt="A screenshot displaying a query using the compare with function"
    src={queriesnrql3tutorial6}
/>
    </Side>
</SideBySide>
    </Step>
    <Step>
To map the comparison of values over time, add TIMESERIES. This creates a line chart of the comparison, allowing you to visualize how this period compares to recent data, and track it over time.

```sql
SELECT average(duration) 
FROM Public_APICall 
SINCE 1 day ago 
COMPARE WITH 1 week ago 
TIMESERIES
```
<img
    title="Compare time windows with timeseries"
    alt="A screenshot displaying a query using the compare with function using a timeseries"
    src={queriesnrql3tutorial7}
/>
    </Step>
    <Step>
You can also specify many different relative time periods in the same format, similar to UNTIL. For instance, you may specify 4 WEEKS AGO or 6 HOURS AGO.

```sql
SELECT average(duration) 
FROM Public_APICall 
SINCE 1 hour ago 
COMPARE WITH 6 hours ago 
TIMESERIES
```
<img
    title="Compare time windows using relative time periods"
    alt="A screenshot displaying a query using the compare with function using a relative timeseries"
    src={queriesnrql3tutorial7}
/>
    </Step>
Comparisons can quickly answer questions about what's happening in your applications. Are different sales, performance, MTTR, or error values up or down compared to last week? And, if you are investigating an issue, comparing aperiod of problemantic performance to a period of normal performance can be very useful.
</Steps>

### Use wildcard filters

<Steps>
    <Step>
We now know how to use a WHERE clause to filter the results in our query. Aside from using standard comparison operators, we can also use LIKE and NOT LIKE if we want to determine whether an attribute contains or does not contain a specified substring. In order to achieve this, we need to use the percent (%) symbol as a wildcard anywhere in the string.

In our sample query, we are getting the number of transactions with the term "Web" anywhere (beginning, middle or end) in the name.

<SideBySide>
    <Side>
```sql
SELECT count(*) 
FROM Public_APICall 
WHERE http.url 
LIKE '%amazonaws%' 
FACET http.url 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Wildcard filters"
    alt="A screenshot displaying a query using a wildcard filter"
    src={queriesnrql3tutorial8}
/>
    </Side>
</SideBySide>

    </Step>
    <Step>
If we change our query to use NOT LIKE instead, then we will get the number of transactions that do not contain the word "Web" in its name.

<SideBySide>
    <Side>
```sql
SELECT count(*) 
FROM Public_APICall 
WHERE http.url NOT 
LIKE '%google%' 
FACET http.url 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Wildcard filters using Not like"
    alt="A screenshot displaying a query using a Not like filter"
    src={queriesnrql3tutorial9}
/>
    </Side>
</SideBySide>

    </Step>
    <Step>
We used the wild card % at the beginning and end, which means that we are checking the value of the attribute we chose if it contains "Web" anywhere in the text. Equally, you could use `%Web` OR `Web%` to match something that ends in "Web" or starts with "Web", respectively.

You can also add the wildcard in between strings for a more refined search. This query will check for a transaction name that contains the word "Web" followed by any text, but then also contains the word "index" followed by any number of characters. So, the results will only be transactions with "Web" and "index" in the name.        

<SideBySide>
    <Side>
```sql
SELECT count(*) 
FROM Public_APICall 
WHERE http.url NOT 
LIKE '%amazon%.com' 
FACET http.url 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Wildcard filters using %"
    alt="A screenshot displaying a query using a % filter"
    src={queriesnrql3tutorial10}
/>
    </Side>
</SideBySide>

    </Step>
    <Step>
What if you need to be extremely specific and the names don't have a common string you can match using wildcards. The IN and NOT IN operators allow us to specify a set of values that we would like to check against an attribute. Instead of specifying multiple WHERE clauses with AND or OR operators, we can simplify our condition by listing the values in parentheses separated by commas.

In this sample query, we are counting the number of transactions whose subtype is either "SpringController" or "StrutsAction". If you change our query to use NOT IN instead, then we will get the number of transactions whose subtype is neither "SpringController" nor "StrutsAction".

<SideBySide>
    <Side>
```sql
SELECT count(*) 
FROM Public_APICall 
WHERE http.url IN ('graph.microsoft.com', 's3.amazonaws.com') 
SINCE 1 day ago
```
    </Side>
    <Side>
<img
    title="Wildcard filters using In"
    alt="A screenshot displaying a query using an In filter"
    src={queriesnrql3tutorial11}
/>
    </Side>
</SideBySide>
    </Step>
</Steps>

You can now control your data and manipulate it to do what you need, allowing you to construct powerful, meaningful dashboards and alerts.

### Specify time ranges

<Steps>
    <Step>
The SINCE and UNTIL clauses are not limited to relative time ranges. You can also provide a specific date and/or time. In the following sample query, we include a SINCE date in YYYY-MM-DD format. This is useful when creating SLA reports for a specified period of time.

```sql
SELECT average(duration) 
FROM Public_APICall SINCE '2023-10-28' 
TIMESERIES MAX
```
<img
    title="Specify a time range"
    alt="A screenshot displaying a time range using since"
    src={queriesnrql3tutorial12}
/>
    </Step>
    <Step>
You can even include specific time with the format YYYY-MM-DD HH:MM. In this query, you can see the data is set at 6pm.

```sql
SELECT average(duration) 
FROM Public_APICall SINCE '2023-10-28 18:00' 
TIMESERIES MAX
```
<img
    title="Specify a unique time range"
    alt="A screenshot displaying a specific time range using YYYY-MM-DD HH:MM"
    src={queriesnrql3tutorial13}
/>       
    </Step>
    <Step>
Sometimes, as an engineer, you might receive an event time in epoch (unix) time. Conveniently, epoch timestamps are an acceptable value in SINCE and UNTIL clauses, so you don't have to translate these values into another date format.

```sql
SELECT average(duration) 
FROM Public_APICall SINCE 1698525489519 UNTIL 1698698289519 
TIMESERIES MAX
```
<img
    title="Specify a time range using unix"
    alt="A screenshot displaying a specific time range using unix"
    src={queriesnrql3tutorial14}
/>       
    </Step>
    <Step>
When NRDB shows data over a period of time, it assumes you want to see the data from the perspective of your timezone. But with dispersed international teams, your "today" could be someone else's "tomorrow" or "yesterday" depending on their location.

You can use the WITH TIMEZONE clause to define a timezone to display data from. This affects the interpretation of values in SINCE and UNTIL clauses.

Consider the two example charts below. Each query has a specified timezone using WITH TIMEZONE. The two are 8 hours apart. Notice the pattern of data is the same, but offset 8 hours to align with each respective timezone:

```sql
SELECT count(*) 
FROM Public_APICall 
SINCE yesterday 
UNTIL today WITH TIMEZONE 'America/Los_Angeles' 
TIMESERIES
```
<img
    title="Specify a time range with timezone"
    alt="A screenshot displaying a specific time range using with timezone"
    src={queriesnrql3tutorial15}
/>     

```sql
SELECT count(*) 
FROM Public_APICall 
SINCE yesterday 
UNTIL today WITH TIMEZONE 'Europe/London' 
TIMESERIES
```
<img
    title="Specify a time range with a specific timezone"
    alt="A screenshot displaying a specific time range using with the Europe/London timezone"
    src={queriesnrql3tutorial16}
/>  
    </Step>
</Steps>

Before this lesson, all our time control mechanisms were based on relative times from now. With the lessons today, we have learned how to adjust the view depending where someone is in the world. Maybe a customer in East Coast America reports an issue and your engineering team is in West Coast America. They can build a dashboard and translate the view to map to the timezone as a customer would be citing. So if a customer advises an issue at 9am East Coast, we can ensure when we look at 9am we don't have to mentally translate.

Epoch and standard date stamps make sense. So when you need to focus your data to specific dates of an incident for example, and you want to investigate the data without a moving time window being relative to the current time, this knowledge will help you obtain data in a static time window.

### Using time-based cohorting

<Steps>
    <Step>
Time-based cohorting...what a complicated but fun term! While it sounds complex, "time-based cohorting" is simply a way to organize data into time-based groups like minuteOf, hourOf, weekOf, and more.

When we use the SINCE clause for durations, we retrieve the entire length of time queried for. But that data may not always tell the whole story! We may need to ananlyze performance within a time period more closely. With time-based cohorting, we can further sort the data into logical, time-based groupings.

Using a combination of FACET and one of the many time-based functions (such as hourOf(timestamp)) we can take a week’s worth of data and understand performance based on the specific hour it occurred. This reveals trends and identifies the most critical times for our application. Check it out:

```sql
SELECT average(duration) 
FROM Public_APICall 
FACET hourOf(timestamp) 
SINCE 1 week ago
```
<img
    title="Time-based cohorting"
    alt="A screenshot displaying the use of time-based cohorting using Facet and hour of functions"
    src={queriesnrql3tutorial17}
/>  
    </Step>
    <Step>
After running the query above we can see the slowest response time based on the hour of the day. Super interesting, right?

New Relic provides many different options to facet based on time. The previous example is grouped by the hour, but we can also group by the day of the week to determine which days have the best and worst response times.

```sql
SELECT average(duration) 
FROM Public_APICall 
FACET weekdayOf(timestamp) 
SINCE 1 week ago
```
<img
    title="Time-based cohorting with weekdayOf"
    alt="A screenshot displaying the use of time-based cohorting using Facet and weekday of functions"
    src={queriesnrql3tutorial18}
/>  
    </Step>
    <Step>
Now can see when are we slowest on any specific day. This could be logically extended to answer business-critical questions like "When do we sell the most products?", or "When do we have the most sign-ups or logins?".

You can also group results by a specific date. This helps when considering SLA reports, or analyzing performance changes over a given period.

```sql
SELECT average(duration) 
FROM Public_APICall 
FACET dateOf(timestamp) 
SINCE 1 week ago
```
<img
    title="Time-based cohorting with dateOf"
    alt="A screenshot displaying the use of time-based cohorting using Facet and ate of functions"
    src={queriesnrql3tutorial19}
/>        
    </Step>
</Steps>

Digging into your data is always fun! Time-based cohorting is an easy way to expose problems that occur on specific minutes, hours, days, or weeks. No matter what data you send to New Relic, NRQL allows you to slice, dice, organize, and visualize it however you want.

There are also many other options available to group by, including week, month, and year depending on your data retention. To see the full list, head to our [Group Results Across Time documentation Page](/docs/query-data/nrql-new-relic-query-language/nrql-query-examples/group-results-across-time).

<CollapserGroup>
    <Collapser
        id="summary"
        title="Lesson summary"
    >
With the knowledge you've gained here, you can create dashboard visualizations, and control the aspects of your data you're most interested in. These techniques are also extremely useful in narrowing focus for more granular, specific alerts!

You are now a Level 2 NRQL hero. You've learned great techniques to control your data and produce much more interesting visualizations. Get ready to advance to level three, where you'll learn more interesting NRQL skills including filters, facet cases, histogram, apdex, filtering to eventTypes, overriding values, and extrapolation.

Revel in your newly attained knowledge. You are now equipped to build really powerful dashboards and alerts that could be critical to your organization.
    </Collapser>
</CollapserGroup>

### What's next?

Content forthcoming