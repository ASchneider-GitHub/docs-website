---
title: Create alert conditions
tags:
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition
  - /docs/alerts/alert-policies/configuring-alerts/managing-your-alerts
  - /docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/alert-conditions
  - /docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/select-product-targets-alert-condition
  - /docs/alerts/create-alert/create-alert-condition/create-alert-conditions
  - /docs/alerts/create-alert/create-alert-condition/update-or-disable-policies-conditions
  - /docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-alert-conditions
freshnessValidatedDate: 2024-10-28
---

An alert condition is the core element that defines when an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/incident-event-attributes/#definition) is created. It acts as the essential starting point for building any meaningful alert. Alert conditions contain the parameters or thresholds met before you're informed.  They can mitigate excessive alerting or tell your team when new or unusual behavior appears.

An alert condition is a continuously running query that measures a given set of events against a defined threshold and opens an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) when the threshold is met for a specified window of time.

There are a lot of ways to create an alert condition. You can create an alert condition from:

* A [chart](#create-chart)
* Alert [policies](#create-policy)
* The [<DNT>**Alert coverage gaps**</DNT>](/docs/alerts/create-alert/alert-coverage-gaps/#create-an-alert) option
* The [<DNT>**Use guided mode**</DNT>](#create-guided-mode) option in the UI
* The [<DNT>**Write your own query**</DNT>](#create-own-query) option in the UI


<Steps>
  <Step>
    <Tabs>
      <TabsBar>
        <TabsBarItem id="create-chart">
          Create from a chart
        </TabsBarItem>

        <TabsBarItem id="create-policy">
          Create from alert policies
        </TabsBarItem>

        <TabsBarItem id="create-guided-mode">
          Create from the guided mode
        </TabsBarItem>

        <TabsBarItem id="create-own-query">
          Create writing your own query
        </TabsBarItem>
      </TabsBar>

      <TabsPages>
        <TabsPageItem id="create-chart">
        You can create an alert condition from pre-existing NRQL queries that are part of a chart.

        To create a new alert condition from a chart, follow these steps:

        1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Dashboards**</DNT> and select a dashboard.

        2. Find a chart you want to use to create your alert condition, click the <Icon name="fe-more-horizontal"/> icon on the right corner of the chart, and select <DNT>**Create alert condition**</DNT>.

        <img
          width="70%;"
          title="Create an alert condition from a chart"
          alt="Create an alert condition from a chart"
          src="/images/alerts_screenshot-crop_chart-create-alert-condition.webp"
        />

        3. The create new alert condition page opens having a specific query. <DNT>**Run**</DNT>.

        4. Review your NRQL query and click <DNT>**Next**</DNT>.      

        </TabsPageItem>

        <TabsPageItem id="create-policy">

        You can create a new alert condition from alert policies.

        To create an alert condition from alert policies, follow these steps:

        1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts**</DNT>.

        2. Select <DNT>**Alert Policies**</DNT> in the left navigation.

        3. Click <DNT>**+ New alert condition**</DNT>.

        <img
          title="Create an alert condition from alert policies"
          alt="Create an alert condition from alert policies"
          src="/images/alerts_screenshot-crop_alert-policies-create-alert-condition.webp"
        />

        4. Select one of these options:

            * <DNT>**[Use guided mode](#create-guided-mode)**</DNT>

            * <DNT>**[Write your own query](#create-own-query)**</DNT>

        </TabsPageItem>

        <TabsPageItem id="create-guided-mode">
        When you create a new alert condition using the guided mode, you'll have several options that you'll need to choose. From your selected options, we'll build your query. We recommend this option becauseâ€¦.

        To create an alert condition using the guided mode, follow these steps:

        1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts**</DNT>.

        2. Select <DNT>**Alert Conditions**</DNT> in the left navigation.

        3. Click <DNT>**+ New alert condition**</DNT>.

        4. Select <DNT>**Use guided mode**</DNT>.
                
        <img
          width="50%;"
          title="Create an alert condition using the guided mode"
          alt="Create an alert condition using the guided mode"
          src="/images/alerts_screenshot-crop_conditions-guided-mode-option.webp"
        />

        5. Select the part or parts of your system you want to include in your alert condition.

        6. Click <DNT>**Next**</DNT>.    

        7. Select the entities to watch.

        8. Select a metric to monitor. Depending on the selected parts of your system, you'll see different metrics. These are the usual:

            * [Golden metrics](/docs/apis/nerdgraph/examples/golden-metrics-entities-nerdgraph-api-tutorial/)

            * Other metrics
      
          If you selected Host, you'll see these metrics:
            * Golden metrics
            * Host metrics
            * Host not reporting
            * Storage metrics
            * Network metrics
            * Process metrics

          If you selected Synthetic monitors, you'll see these metrics:
            * Median duration (s)
            * Failures

        9. Review your NRQL query.

        10. Click <DNT>**Next**</DNT>.

      </TabsPageItem>

      <TabsPageItem id="create-own-query">
      This option allows you to use NRQL to define your alert from scratch.

      To create an alert condition writing your own query, follow these steps:

      1. Go to **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts**.

      2. Select **Alert Conditions** in the left navigation.

      3. Click **New alert condition**.

      4. Select **Write your own query**.

      <img
          width="50%;"
          title="Create an alert condition writing your own query"
          alt="Create an alert condition writing your own query"
          src="/images/alerts_screenshot-crop_conditions-writing-own-query.webp"
      />

      5. Select the part or parts of your system you want to include in your alert condition.

      6. Click **Next**.

      7. Write your query and click **Run**.

      8. Review your NRQL query and click **Next**.

      </TabsPageItem>
      </TabsPages>
    </Tabs>
  </Step>

  <Step>

    ### Set thresholds for alert conditions [#thresholds]

    A [threshold](/docs/new-relic-solutions/get-started/glossary/#alert-threshold) is a value that you define in your alert condition. Thresholds are the rules each alert condition must follow. When this defined value is reached for a specified window of time, an [incident](/docs/new-relic-solutions/get-started/glossary/#alert-incident) is created. An incident means there is a problem with your system and you should investigate.

      <img
          title="Set thresholds for alert conditions"
          alt="Set thresholds for alert conditions"
          src="/images/alerts_screenshot-crop_conditions-set-thresholds.webp"
      />

    <CollapserGroup>
      <Collapser
        id="window-duration"
        title="Window duration"
      >

      Setting the window duration for your alert condition tells New Relic how to group your data. If you're creating an alert condition for a data set that sends a signal to New Relic once every hour, you'd want to set the window duration to something closer to sixty minutes because it'll help spot patterns and unusual behavior. But, if you're creating an alert condition for web transaction time and New Relic collects a signal for that data every minute, we'd recommend setting the window duration to one minute.

      For your first alert we recommend sticking with our default settings, but the more you get familiar with creating an alert condition we encourage you to customize these fields based on your own experience.

      </Collapser>

      <Collapser
        id="sliding-window"
        title="Use sliding window aggregation"
      >
        Throughout the day, data streams from your application into New Relic. Instead of evaluating that data immediately for incidents, alert conditions collect the data over a period of time known as the <DNT>**aggregation window**</DNT>. An additional delay allows for slower data points to arrive before the window is aggregated.

        Sliding windows are helpful when you need to smooth out "spiky" charts. One common use case is to use sliding windows to smooth line graphs that have a lot of variation over short periods of time in cases where the rolling aggregate is more important than aggregates from narrow windows of time.

        We recommend using our sliding window aggregation if you're not expecting to have a steady and consistent stream of data but are expecting some dips and spikes in data.

      </Collapser>

      <Collapser
        id="streaming-method"
        title="Streaming method"
      >
        In general, we recommend using the <DNT>**event flow**</DNT> streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where <DNT>**event timer**</DNT> might be a better method to choose, but for your first alert we recommend our default, <DNT>**event flow**</DNT>. To better understand which streaming method to choose, see [Streaming alerts: key terms and concepts](/docs/alerts/create-alert/fine-tune/streaming-alerts-key-terms-concepts/#aggregation-methods).

    </Collapser>

    <Collapser
      id="timer"
      title="Timer"
    >
      This field indicates how long we need to wait after each data point to make sure sure we've processed the entire bach.

      Note that if your timer is much shorter than your window duration and your data flow is inconsistent, your alerts may not be accurate.

    </Collapser>

    <Collapser
      id="gap-filling-strategy"
      title="Gap filling strategy"
    >

    Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings:

    * <DNT>**None**</DNT>: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be an incident.

    * <DNT>**Custom static value**</DNT>: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of `fillValue` (as named in the API) that specifies what static value should be used. This defaults to `0`.
    
    * <DNT>**Last known value**</DNT>: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for a minimum of 2 hours. If the configured threshold duration is longer than 2 hours, this value is kept for that duration instead.

    </Collapser>

    <Collapser
      id="evaluation-delay"
      title="Evaluation delay"
    >
    Evaluation delay is how long we wait before we start evaluating a signal agains the thresholds in this condition. You can enable the `Use evaluation delay` flag and set up to 120 minutes to delay the evalution of incoming signals.

    When new entities are first deployed, resource utilization on the entity is often unusually high. In autoscale environments this can easily create a lot of false alerts. By delaying the start of alert detection on signals emitted from new entities you can significantly reduce the number of false alarms associated with deployments in orchestrated or autoscale environments.

    </Collapser>

    <Collapser
      id="anomaly-threshold"
      title="Anomaly threshold"
    >
    Anomaly thresholds are ideal when you're more concerned about deviations from expected patterns than specific numerical values. They enable you to monitor for unusual activity without needing to set predefined limits. New Relic's anomaly detection dynamically analyzes your data over time, adapting thresholds to reflect evolving system behavior.

    Setting up anomaly detection:

        1. Choose upper or lower:
           * **Upper and lower** to be alerted about any higher and lower deviations than expected.
           * **Lower only**: 
           * **Upper only**: To focus solely on unusually high values.
        2. Assign security level:
           * **Critical**: Set the priority level to critical for your initial alert to ensure prompt attention to potential issues.
           * **Warning**:
        3. **When a query returns a value outside the threshold**

      You can learn more about priority levels in our [alert condition docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels). You can also check our documentation about [anomaly threshold and model behaviors](/docs/alerts/create-alert/set-thresholds/anomaly-detection/).
      </Collapser>

      <Collapser
        id="static-threshold"
        title="Static threshold"
      >
        Unlike anomaly thresholds, a static threshold doesn't look at your data set as a whole and determines what behavior is unusual based on your system's history. Instead, a static threshold will open an incident whenever your system behaves differently than the criteria that you set.

        You need to set the priority level for both anomaly and static thresholds. See the section above for more details.
      </Collapser>

      <Collapser
        id="lost-signal"
        title="Add lost signal threshold"
      >
        You can use the **Consider the signal lost after** option to adjust the time window from 30 seconds to 48 hours. The [lost signal threshold](/docs/alerts/create-alert/create-alert-condition/create-nrql-alert-conditions/#signal-loss) determines how long to wait before considering a missing signal lost. If the signal doesn't return within that time, you can choose to open a new incident or close any related ones. You can also choose to skip opening an incident when a signal is expected to terminate. Set the threshold based on your system's expected behavior and data collection frequency. For example, if a website experiences a complete loss of traffic, or throughput, the corresponding telemetry data sent to New Relic will also cease. Monitoring for this loss of signal can serve as an early warning system for such outages.
      </Collapser>
    </CollapserGroup>    
    
    </Step>
    <Step>
    ### Add alert condition details [#add-details]

      <img
          title="Add alert condition details"
          alt="Add alert condition details"
          src="/images/alerts_screenshot-crop_condition-add-details.webp"
      />

    <CollapserGroup>
      <Collapser
        id="name-your-condition"
        title="Name your alert condition"
      >
        A best practice for condition naming involves a structured format that conveys essential information at a glance. Include the following elements in your condition names:

        * **Priority**: Indicate the severity or urgency of the alert, like P1, P2, P3.
        * **Signal**: Specify the metric or condition being monitored, like High Avg Latency or Low Throughput.
        * **Entity**: Identify the affected system, application, or component, like WebPortal App or Database Server.


      An example of a well-formed condition name following this structure would be `P2 | High Avg Latency | WebPortal App`.
      </Collapser>

      <Collapser
        id="connect-condition-policy"
        title="Connect this condition to a policy"
      >
        If you already have a policy you want to connect to an alert condition, then select the existing policy. Learn more about policies [here](/docs/alerts/organize-alerts/create-edit-or-find-alert-policy/).

        If you prefer to create a new policy, you'll have these options:

        * **Policy name**: Type a [meaningful name](/docs/alerts/organize-alerts/create-edit-or-find-alert-policy/#best-practices-policies) for the policy (maximum 64 characters).

        * **Group incidents into issues**: You have to choose an issue preference option. See [Issue preference options](/docs/alerts/organize-alerts/specify-when-alerts-create-incidents/#preference-options) for more information about it.


      Check the box <DNT>**Correlate and suppress noise**</DNT> to enable [correlation](/docs/alerts/organize-alerts/change-applied-intelligence-correlation-logic-decisions/#configure-correlation) for the alert policy and only get notified when you need to take action.

      </Collapser>
      <Collapser
        id="close-open-incidents"
        title="Close open incidents after"
      >
        An incident automatically closes when the targeted signal returns to a non-breaching state for the period indicated in the condition's thresholds. This wait time is called the recovery period.

        When an incident closes automatically:

        1. The closing timestamp is backdated to the start of the recovery period.
        2. The evaluation resets and restarts from when the previous incident ended.


      All conditions have an incident time limit setting that automatically force-close a long-lasting incident. New Relic automatically defaults to 3 days and recommends that you use our default settings for your first alert. Another way to close an open incident when the signal does not return data is by configuring a [`loss of signal`](/docs/alerts/create-alert/create-alert-condition/create-nrql-alert-conditions/#signal-loss) threshold.

      </Collapser>

      <Collapser
        id="customize-incidents"
        title="Customize incidents from this condition"
      >

      A [title template](/docs/alerts/create-alert/condition-details/title-template) is used when incidents are opened by the condition. It overrides the default title. Your title should use handlebars for incident event attributes. For example, `{{conditionName}}` targeting `{{targetName}}` incident.

      You can use the **Description template** field to define a description template with tags and custom attributes such as host name, owning team, and product, to consistently pass useful information downstream.
    </Collapser>
      <Collapser
        id="runbook"
        title="Runbook URL"
      >
        If you'd like to link to a runbook for the condition that triggered the incident, you can add the URL in the runbook URL field.
      </Collapser>
    </CollapserGroup>    

    </Step>
    <Step>
          ### Save your alert condition

      Once you've finished, click **Save condition**. You'll see a summary of your alert condition.    
    </Step>
</Steps>

