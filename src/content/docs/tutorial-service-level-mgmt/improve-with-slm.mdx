---
title: "Improve with service level management"
metaDescription: "New Relic observability maturity series: our service level management guide shows you how to easily measure and communicate the overall health, performance, and quality of your digital products and services to all stakeholders."
redirects:
freshnessValidatedDate: never
---

## Improvement process [#improvement-process]

### Alert quality management

[Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide) is another observability maturity practice that marries really well with service level management. The value of both alerting quality data side-by-side with service level data is that you can see if alert policies are aligned with real impact or are just creating noise. You'll be able to validate good alerts, missing alerts, and just noisy alerts.

You can do this by creating a custom dashboard with an SLI compliance query side-by-side with an alerting quality query.

If you haven't already, check out our [Alert quality management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide).

### Adoption and continuous improvement

Improvement of service levels and reliability requires adoption of the practice by all the stakeholders of the service. This includes, but is not limited to, engineering management, product management, and executive management. The primary goal is to quickly demonstrate the power and value of service levels to stakeholders in order to start a meaningful discussion on what really matters to those stakeholders. The steps in this guide will get you those meaningful discussions very quickly.

A proven method, with a high rate of adoption, is to first establish output performance and input performance service levels for one digital product and its critical capabilities. This usually involves one overall output and input service level for each endpoint application (usually one or two), and then approximately 4-7 output performance service levels for assumed critical capabilities measured at the endpoint transaction.

This method includes **not** surveying each stakeholder for what should and shouldn't be measured. Surveys usually result in long wait times, lots of questions, frustration, lack of demonstrated value, and suboptimal answers. Remember, start with baselines and key transactions as "capabilities."

Freely make assumptions of what these endpoints are, and what endpoint transactions make up what capabilities, as demonstrated above. Accuracy is not the key at first. What is key to a successful kick-off is demonstrating the ability to easily measure and communicate health. That initial demonstration will show the value in investing more time to refine what is and what isn't measured in primary service levels.

Don't wait. The sooner you provide that demonstration and the more complete that demonstration is, the sooner you will achieve broader adoption and begin the reliability improvement process in collaboration with all the stakeholders!

### Automation

Once you have established what works (and what doesn't) for your stakeholders, you can then begin to design SLM at scale with automation. You can start learning about automating service level management by studying the [New Relic Terraform library](https://registry.terraform.io/providers/newrelic/newrelic/latest/docs/resources/service_level).

## Business value

As stated in the "[Desired outcomes](#desired-outcomes)" section above; the preverbial bottom-line result is to reduce the cost of business impacting incidents.

However, service levels can also help quantify both estimated revenue loss during incidents as well as estimated revenue at risk for subscription-based businesses.

**Revenue loss** can be easily estimated for revenue generated by transaction, such as online retail, as well as penalties paid if your business has service level agreement contracts with penalties built-in.

**Revenue at risk** is for subscription-based (SaaS) business models where each customer has a monthly or annual subscription value. You can easily estimate the number of customers impacted and their subscription revenue by period to calculate "revenue at risk." Note: subscription businesses can also have penalties within a service level agreement contract, which should be included as stated below.

### Quantifying the direct cost of service level agreement breaches

Determine the cost of previous breaches. For example, online retail businesses know the estimated revenue loss per minute during service loss (downtime). Legal can tell you the penalty costs of service level agreement (SLA) contract breaches. Both losses can be easily *estimated* in real-time using New Relic data on service level breaches.

### Quantifying the revenue opportunity costs of service level breaches

Determine the three variables below.

* (A) number of breaches that trigger penalties or revenue loss
* (B) average duration of breaches
* (C) average penalty or revenue loss per minute/hour

Multiply those three variables (A * B * C) to calculate total revenue opportunity to recover.

### Quantifying revenue leakage

Determine the two variables below.

* (A) Total revenue (per period)
* (B) Total penalties payments made to customers (per same period as A)

Divide B / A to calculate revenue leakage % rate.

## Tracking your service level objectives [#tracking-slo]

Service levels are a practice, just like testing, alerting, game days, and other recurring practices. You could think of them as a scientific instrument that helps you to measure the "health" of your systems. But like all tools, service levels require calibration.

Include the service level practice in your team's process. The following recommendations were distilled from the experience using service levels within teams at New Relic, and you should adjust them for your specific team requirements:

- Do a periodic review of the service levels, and pay close attention to:
    + Do the SLIs reflect incidents and pages?
    + What's your error budget for a week?
        * If it's too low, investigate what caused a drop, using the "Analyze" feature to find bad events that caused it,
        * If it's 100%, make sure your indicator is correct and the SLO is aggressive enough. Being at 100% indicates the SLO is too safe.
        * What is the trend that you observe in various time periods (1d/7d/28d).
- Keep an eye on SLIs during game days. SLIs should reflect the impact, just like your alerts do.
- When you have a drop of error budget on production, evaluate why it didn't happen on staging.

## Next steps [#next-steps]

The next step in our observability maturity practice is to add in customer experience service levels measured at the client browser or mobile device. Again, it's important you first prove value as described in the improvement process above. Remember: observability is a journey, and maturity takes time, practice, and patience.

To proceed on your journey, see:
* Our guide on [Customer experience: Quality foundation](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).
* If you haven't already, see our guide on [Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide).

<UserJourneyControls
    previousStep={{path: "/docs/tutorial-service-level-mgmt/establish-capability-sli/", title: "Previous step", body: "Learn how to create service level scores on capabilities"}}
/>
