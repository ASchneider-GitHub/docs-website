---
title: 'View AI model data in New Relic'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

INTRO: Describe how New Relic can help you track the performance of models in your toolchain

SCREENSHOT, UI

## Model inventory page [#model-inventory]

INTRO: Focus on overview, highest level (models-focused, performance, kind of data). Differentiate model inventory by saying that model inventory takes a vertical look at AI data whereas response overview looks at it horizontally. 

IMAGE: Ask moran for that diagram, redo it to make it make sense

DIFFERENT SCREENSHOT, UI

UI Tab 1: Describe briefly, one sentence
UI Tab 2: Describe briefly, one sentence
UI Tab 3: Describe briefly, one sentence
UI Tab 4: Describe briefly, one sentence

Discuss an example scenario briefly about how model inventory can tell you about completions made to models in your entities. 

## Model comparison page [#model-comparison]

The model comparison page gives you the flexibility to analyze performance depending on the use case you're testing for. You can:


Compare how one model performs within an app against the average performance across all services.
Conduct A/B tests when testing different prompts during prompt engineering. For example, comparing a model's performance and accuracy during one time window with one set of prompts against another time window with a second set of prompts.
Evaluate how a model performed during a specific time window when customer traffic peaked.

Keep in mind that this page scopes your model comparison data to a single account. If your organization has multiple accounts that own several AI-powered apps, you wouldn't be able to compare model data between those accounts.

### Understand model cost [#understand-model-cost]

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.
Tokens per completion: The token average for all completion events.
Prompt tokens: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
Completion tokens: The number of tokens consumed by the model when it generates the response delivered to the end user.
When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.



