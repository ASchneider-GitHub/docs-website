---
title: 'Analyze model performance and cost data'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiIntrotoModelData from 'images/ai_screenshot-crop_intro-to-model-data.webp'

import aiModelInventoryOverviewPage from 'images/ai_screenshot-full_model-inventory-overview-page.webp'

import aiModelInventoryErrors from 'images/ai_screenshot-crop_model-inventory-errors.webp'

import aiModelInventoryPerformancePage from 'images/ai_screenshot-full_model-inventory-performance-page.webp'

import aiModelComparisonPage from 'images/ai_screenshot-full_ai-model-comparison-page.webp'

import aiModelComparisonCost from 'images/ai_screenshot-crop_model-comparison-cost.webp'

import aiModelComparisonPerformance from 'images/ai_screenshot-crop_model-comparison-performance.webp'

AI monitoring simplifies how you analyze the model layer of your AI toolchain. You can locate errors or monitor performance metrics specific to the completions, tokens, and requests made to your AI model. You can find model data in two areas:

* **Model inventory**: Isolate token usage, keep track of overall performance, or dig into the individual completions your models make.
* **Compare models**: Analyze data scoped to any two models, letting you evaluate relative performance and cost over time.

<img
    title="Model data overview"
    alt="A screenshot of the model inventory page"
    src={aiIntrotoModelData}
/>
<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring</DoNotTranslate>**: From AI monitoring, you can choose between model inventory or model comparison.
</figcaption>

## Model inventory page [#model-inventory]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelInventoryOverviewPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DoNotTranslate>**: View data about interactions with your AI model.
</figcaption>

The model inventory page surfaces AI model data from the AI app layer. From the overview page, you can explore the number of requests made to a model against its response times, or you can analyze time series graphs that show you how your model's behavior changes over time.

By scoping to the model itself, you can get a better sense of how the model processes and returns certain outputs. For example, your model may process multiple completions or make several calls to other services before your end user receives a response. Model inventory exposes these processes, showing you where and when changes occur in a response cycle.

### Errors page [#errors-inventory]

<img
    title="Model inventory: Errors"
    alt="A screenshot of the Errors time series and chart"
    src={aiModelInventoryErrors}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Errors</DoNotTranslate>**: View data about AI model errors.
</figcaption>

The errors page uses time series graphs and tables to organize errors that appear from your AI models. 

* **Response errors:** Track the number of errors in aggregate that come from your AI model. For example, if there was a spike in total errors during a time period, you can see that spike in the Response errors time series.
* **Response errors by model**: From the total number of errors, you can see if there is one model in particular that's producing more errors than the average.
* **Response errors by type**: You can view when certain error codes appear. For example, if one or several of your model's has reached rate limits for requests, this time series will tell you when and how many times that incident has occurred. 

Beneath the time series graphs, you can view the errors table. This table organizes request and responses from your model by error, including information about the error type and error message. 

### Performance page [#performance-inventory]

<img
    title="Model inventory: Performance"
    alt="A screenshot of the Errors time series and chart"
    src={aiModelInventoryPerformancePage}
/>
<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance</DoNotTranslate>**: View data about your AI model's performance.
</figcaption>

The performance page segments data based on aggregated response and request time across all your models. You can use this page to determine if an increase in requests or responses in a given period led to an uptick in latency. 

### Cost page [#cost-inventory]

## SCREENSHOT HERE

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Cost</DoNotTranslate>**: View data about your AI model's cost.
</figcaption>

The cost page uses a combination of time series and charts to help you identify what drives cost in your AI toolchain. Determine whether an increase in cost came from prompts or completions, or if certain models cost more on average than others.  

* **Tokens used and token limit**: Evaluate how often your models approach a given token limit, then make adjustments to how your models generate responses with prompt engineering.
**Total usage by prompt and completion tokens**: Understand what ratio of tokens comes from prompts your model accepts against tokens used per completion. 

## Model comparison page [#model-comparison]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelComparisonPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page organizes your AI monitoring data to help you conduct comparative analysis. This page scopes your model comparison data to a single account, giving you aggregated data about model cost and performance across one or more apps. To generate data:

1. Choose your models from the drop down.
1. Scope to one service to see performance in the context of a particular app, or keep the query to `Service = All` to see how a model behaves on average. 
1. Choose a time parameters. This tool is flexible: you can make comparisons across different time periods, which lets you see how performance or cost changed before and after a deployment.  

### Compare model performance [#compare-performance]

<img
    title="Model comparison page: Performance"
    alt="A screenshot of model comparison"
    src={aiModelComparisonPerformance}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare performance between different AI models in your stack.
</figcaption>

Compare how one model performs within an app against the average performance across all services.

* Conduct A/B tests when testing different prompts during prompt engineering.
* Compare a model's performance and accuracy during one time window against another time window. From there, determine how a change made to your models affected performance over time. 
* Evaluate how a model performed during a specific time window when customer traffic peaked.

### Compare model cost [#compare-cost]

<img
    title="Model comparison page: Cost"
    alt="A screenshot of model comparison"
    src={aiModelComparisonCost}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare cost between different AI models in your stack.
</figcaption>

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.

* **Tokens per completion**: The token average for all completion events.
* **Prompt tokens**: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
* **Completion tokens**: The number of tokens consumed by the model when it generates the response delivered to the end user.

When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.