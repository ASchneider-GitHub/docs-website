---
title: 'View AI model data in New Relic'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiModelInventoryOverviewPage from 'images/ai_screenshot-full_model-inventory-overview-page.webp'

import aiModelInventoryPerformancePage from 'images/ai_screenshot-full_model-inventory-performance-page.webp'

import aiModelComparisonPage from 'images/ai_screenshot-full_ai-model-comparison-page.webp'

With your AI app instrumented, you can explore data scoped to model performance and costs. While AI monitoring can give you insight into the performance of your AI app, it can also surface data specific to the models in your toolchain. 

GIF HERE

The model comparison and model inventory pages let you dig deeper into model costs and performance. You can use these pages to troubleshoot incidents, help you conduct A/B tests, or make data-driven decisions when choosing an AI model vendor. Whatever your use case, AI monitoring provides model data that helps you build stronger, more performant apps. 

## Model inventory page [#model-inventory]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelInventoryOverviewPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DoNotTranslate>**: View data about interactions with your AI model.
</figcaption>

The model inventory page surfaces AI model data from the AI app layer. From the overview page, you can explore the number of requests made to a model against its response times, or you can analyze time series graphs that show you how your model's behavior changes over time.

By scoping to the model itself, you can get a better sense of how the model processes and returns certain outputs. For example, your model may process multiple completions or make several calls to other services before your end user receives a response. Model inventory exposes these processes, showing you where and when changes occur in a response cycle.

### Analyze your models [#analyze-all]

The model inventory page segments your data across errors, performance, and cost. Each page shares similar graphs, tables, and time series with the overview page, but keeps the data specific to what you want to target.

<Tabs>
	<TabsBar>
        <TabsBarItem id="errors-inventory">
            Track response errors by model  
        </TabsBarItem>
        <TabsBarItem id="performance-inventory">
            Monitor AI model performance
        </TabsBarItem>
        <TabsBarItem id="cost-inventory">
            Keep model costs down
        </TabsBarItem>
    </TabsBar>

    <TabsPages>
        <TabsPageItem id="errors-inventory">
<img
    title="Model inventory: Errors"
    alt="A screenshot of the performance page"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Errors</DoNotTranslate>**: View data about AI model errors.
</figcaption>

The errors page uses time series graphs and tables to organize errors that appear from your AI models. 

* **Response errors:** Track the number of errors in aggregate that come from your AI model. For example, if there was a spike in total errors during a time period, you can see that spike in the Response errors time series.
* **Response errors by model**: From the total number of errors, you can see if there is one model in particular that's producing more errors than the average.
* **Response errors by type**: You can view when certain error codes appear. For example, if one or several of your model's has reached rate limits for requests, this time series will tell you when and how many times that incident has occurred. 

Beneath the time series graphs, you can view the errors table. This table organizes request and responses from your model by error, including information about the error type and error message. 

        </TabsPageItem>
        <TabsPageItem id="performance-inventory">
<img
    title="Model inventory: Performance"
    alt="A screenshot of the performance page"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance</DoNotTranslate>**: View data about your AI model's performance.
</figcaption>

The performance page segments data based on response and request time. You can use this page to correlate if an increase in requests or responses in a given period led to an uptick in latency. 

        </TabsPageItem>
        <TabsPageItem id="cost-inventory">
<img
    title="Model inventory: Cost"
    alt="A screenshot of the cost page"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Cost</DoNotTranslate>**: View data about your AI model's cost.
</figcaption>

The cost page uses a combination of time series and charts to help you identify what contributes to token usage. Determine whether an increase in cost came from prompts or completions, or if certain models cost more on average than others.  

        </TabsPageItem>
    </TabsPages>
</Tabs>

## Model comparison page [#model-comparison]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelComparisonPage}
/>

<figcaption>
    Go to <DoNotTranslate>**[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page organizes your AI monitoring data so you can compare models with ease. You can use this page to conduct comparison analysis when making decisions about how a model should be implemented in your AI toolchain. 

<Tabs>
	<TabsBar>
        <TabsBarItem id="performance-comparison">
            Compare model performance
        </TabsBarItem>
        <TabsBarItem id="cost-comparison">
            Compare model cost
        </TabsBarItem>
    </TabsBar>

    <TabsPages>
        <TabsPageItem id="performance-comparison">

SCREENSHOT HERE 

Compare how one model performs within an app against the average performance across all services.

* Conduct A/B tests when testing different prompts during prompt engineering. For example, comparing a model's performance and accuracy during one time window with one set of prompts against another time window with a second set of prompts.
* Evaluate how a model performed during a specific time window when customer traffic peaked.
        </TabsPageItem>
        <TabsPageItem id="cost-comparison">
SCREENSHOT HERE

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.

* **Tokens per completion**: The token average for all completion events.
* **Prompt tokens**: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
* **Completion tokens**: The number of tokens consumed by the model when it generates the response delivered to the end user.

When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.
        </TabsPageItem>
    </TabsPages>
</Tabs>

Keep in mind that this page scopes your model comparison data to a single account. If your organization has multiple accounts that own several AI-powered apps, you wouldn't be able to compare model data between those accounts.



