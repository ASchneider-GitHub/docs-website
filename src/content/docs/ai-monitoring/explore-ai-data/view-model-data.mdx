---
title: 'View AI model data in New Relic'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiIntrotoModelData from 'images/ai_screenshot-crop_intro-to-model-data.webp'

import aiModelInventoryOverviewPage from 'images/ai_screenshot-full_model-inventory-overview-page.webp'

import aiModelInventoryPerformancePage from 'images/ai_screenshot-full_model-inventory-performance-page.webp'

import aiModelComparisonPage from 'images/ai_screenshot-full_ai-model-comparison-page.webp'

AI monitoring simplifies how you analyze the model layer of your AI toolchain. You can locate errors or monitor performance metrics specific to the completions, tokens, and requests made to your AI model. You can find model data in two areas:

* **Model inventory**: Isolate token usage, keep track of overall performance, or dig into the individual completions your models make.
* **Compare models**: Analyze data scoped to any two models, letting you evaluate relative performance and cost over time.

<img
    title="Model data overview"
    alt="A screenshot of the model inventory page"
    src={aiIntrotoModelData}
/>
<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring</DoNotTranslate>**: From AI monitoring, you can choose between model inventory or model comparison.
</figcaption>

## Model inventory page [#model-inventory]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelInventoryOverviewPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DoNotTranslate>**: View data about interactions with your AI model.
</figcaption>

The model inventory page surfaces AI model data from the AI app layer. From the overview page, you can explore the number of requests made to a model against its response times, or you can analyze time series graphs that show you how your model's behavior changes over time.

By scoping to the model itself, you can get a better sense of how the model processes and returns certain outputs. For example, your model may process multiple completions or make several calls to other services before your end user receives a response. Model inventory exposes these processes, showing you where and when changes occur in a response cycle.

### Analyze your models [#analyze-all]

The model inventory page segments your data across errors, performance, and cost. Each page shares similar graphs, tables, and time series with the overview page, but keeps the data specific to what you want to target.

<Tabs>
	<TabsBar>
        <TabsBarItem id="errors-inventory">
            Track response errors by model  
        </TabsBarItem>
        <TabsBarItem id="performance-inventory">
            Monitor AI model performance
        </TabsBarItem>
        <TabsBarItem id="cost-inventory">
            Keep model costs down
        </TabsBarItem>
    </TabsBar>

    <TabsPages>
        <TabsPageItem id="errors-inventory">
<img
    title="Model inventory: Errors"
    alt="A screenshot of the performance page"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Errors</DoNotTranslate>**: View data about AI model errors.
</figcaption>

The errors page uses time series graphs and tables to organize errors that appear from your AI models. 

* **Response errors:** Track the number of errors in aggregate that come from your AI model. For example, if there was a spike in total errors during a time period, you can see that spike in the Response errors time series.
* **Response errors by model**: From the total number of errors, you can see if there is one model in particular that's producing more errors than the average.
* **Response errors by type**: You can view when certain error codes appear. For example, if one or several of your model's has reached rate limits for requests, this time series will tell you when and how many times that incident has occurred. 

Beneath the time series graphs, you can view the errors table. This table organizes request and responses from your model by error, including information about the error type and error message. 

        </TabsPageItem>
        <TabsPageItem id="performance-inventory">
<img
    title="Model inventory: Performance"
    alt="A screenshot of the performance page"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance</DoNotTranslate>**: View data about your AI model's performance.
</figcaption>

The performance page segments data based on response and request time. You can use this page to correlate if an increase in requests or responses in a given period led to an uptick in latency. 

        </TabsPageItem>
        <TabsPageItem id="cost-inventory">
<img
    title="Model inventory: Cost"
    alt="A screenshot of the cost page"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Cost</DoNotTranslate>**: View data about your AI model's cost.
</figcaption>

The cost page uses a combination of time series and charts to help you identify what contributes to token usage. Determine whether an increase in cost came from prompts or completions, or if certain models cost more on average than others.  

        </TabsPageItem>
    </TabsPages>
</Tabs>

## Model comparison page [#model-comparison]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelComparisonPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page organizes your AI monitoring data to help you conduct comparative analysis. This page scopes your model comparison data to a single account, giving you aggregated data about model cost and performance across one or more apps. To generate data:

1. Choose your models from the drop down.
2. Scope to one service to see performance in the context of a particular app, or keep the query to `Service = All` to see how a model behaves on average. 
3. Choose a time parameters. This tool is flexible: you can make comparisons across different time periods, which lets you see how performance or cost changed before and after a deployment.  

<Tabs>
	<TabsBar>
        <TabsBarItem id="performance-comparison">
            Compare model performance
        </TabsBarItem>
        <TabsBarItem id="cost-comparison">
            Compare model cost
        </TabsBarItem>
    </TabsBar>

    <TabsPages>
        <TabsPageItem id="performance-comparison">

SCREENSHOT HERE 

Compare how one model performs within an app against the average performance across all services.

* Conduct A/B tests when testing different prompts during prompt engineering. For example, comparing a model's performance and accuracy during one time window with one set of prompts against another time window with a second set of prompts.
* Evaluate how a model performed during a specific time window when customer traffic peaked.
        </TabsPageItem>
        <TabsPageItem id="cost-comparison">
SCREENSHOT HERE

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.

* **Tokens per completion**: The token average for all completion events.
* **Prompt tokens**: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
* **Completion tokens**: The number of tokens consumed by the model when it generates the response delivered to the end user.

When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.
        </TabsPageItem>
    </TabsPages>
</Tabs>