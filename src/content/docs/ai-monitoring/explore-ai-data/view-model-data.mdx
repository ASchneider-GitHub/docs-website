---
title: 'Analyze model data'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiIntrotoModelData from 'images/ai_screenshot-crop_intro-to-model-data.webp'

import aiModelInventoryOverviewPage from 'images/ai_screenshot-full_model-inventory-overview-page.webp'

import aiModelInventoryErrors from 'images/ai_screenshot-crop_model-inventory-errors.webp'

import aiModelInventoryPerformancePage from 'images/ai_screenshot-full_model-inventory-performance-page.webp'

import aiModelInventoryCostPage from 'images/ai_screenshot-crop_model-inventory-cost.webp'

import aiModelComparisonPage from 'images/ai_screenshot-full_ai-model-comparison-page.webp'

import aiModelComparisonCost from 'images/ai_screenshot-crop_model-comparison-cost.webp'

import aiModelComparisonPerformance from 'images/ai_screenshot-crop_model-comparison-performance.webp'

AI monitoring surfaces data about your AI models so you can analyze AI model performance alongside AI app performance. You can find data about your AI models in two areas:

* **Model inventory**: Aggregates data about all your AI models so you can isolate token usage, keep track of overall performance, or dig into the individual completions your models make.
* **Compare models**: Lets you scope your data to any two models, letting you evaluate performance and cost relative to other models over time.

<img
    title="Model data overview"
    alt="A screenshot of the model inventory page"
    src={aiIntrotoModelData}
/>
<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring</DoNotTranslate>**: From AI monitoring, you can choose between model inventory or model comparison.
</figcaption>

## Model inventory page [#model-inventory]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelInventoryOverviewPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DoNotTranslate>**: View data about interactions with your AI model.
</figcaption>

The model inventory page surfaces AI model data from the AI app layer. From the overview tab, you can explore the number of requests made to a model against its response times, or you can analyze time series graphs that show you how your model's behavior changes.

By scoping to the model itself, you get a better sense of how the model processes prompts when it makes a completion. For example, your model may process multiple completions or make several calls to other services before your end user receives a response. 

Model inventory exposes these discrete processes, showing you where and when changes occur in a model response cycle.

### Errors tab [#errors-inventory]

<img
    title="Model inventory: Errors"
    alt="A screenshot of the Errors time series and chart"
    src={aiModelInventoryErrors}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Errors</DoNotTranslate>**: View data about AI model errors.
</figcaption>

The errors tab uses time series graphs and tables to organize errors related to your AI models. 

* **Response errors:** Track the number of errors in aggregate that come from your AI model. For example, if there was a spike in total errors during a time period, you can see that spike in the Response errors time series.
* **Response errors by model**: From the total number of errors, you can see if there is one model in particular that's producing more errors than the average.
* **Response errors by type**: You can view when certain error codes appear. For example, if one or several of your model's has reached rate limits for requests, this time series will tell you when and how many times that incident has occurred. 

Beneath the time series graphs, you can view the errors table. This table organizes request and responses from your model by error, including information about the error type and error message. 

### Performance tab [#performance-inventory]

<img
    title="Model inventory: Performance"
    alt="A screenshot of the Errors time series and chart"
    src={aiModelInventoryPerformancePage}
/>
<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance</DoNotTranslate>**: View data about your AI model's performance.
</figcaption>

The performance tab aggregates response and request metrics across all your models. 

* Use the pie charts to overview which of your models takes the most time to process a request or create a response.
* Refer to the time series graphs to see when an uptick in request or response time occurred. 

### Cost tab [#cost-inventory]

<img
    title="Model inventory: Performance"
    alt="A screenshot of the Errors time series and chart"
    src={aiModelInventoryPerformancePage}
/>
<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Cost</DoNotTranslate>**: View data about your AI model's cost.
</figcaption>

The cost page uses a combination of time series graphs and pie charts to identify cost drivers amongst your models. Determine whether an increase in cost came from prompts or completions, or if certain models cost more on average than others.  

* **Tokens used and token limit**: Evaluate how often your models approach a given token limit.
* **Total tokens by models**: Determine which of your models use the most tokens on average.
* **Total usage by prompt and completion tokens**: Understand what ratio of tokens comes from prompts your model accepts against tokens used per completion. 

When you can see how your tokens are used, you can improve how your AI app uses one or more of your models when it generates its final response to an end user. 

## Model comparison page [#model-comparison]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelComparisonPage}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page organizes your AI monitoring data to help you conduct comparative analysis. This page scopes your model comparison data to a single account, giving you aggregated data about model cost and performance across one or more apps. To generate data:

1. Choose your models from the drop down.
1. Scope to one service to see performance in the context of a particular app, or keep the query to `Service = All` to see how a model behaves on average. 
1. Choose a time parameters. This tool is flexible: you can make comparisons across different time periods, which lets you see how performance or cost changed before and after a deployment.  

### Compare model performance [#compare-performance]

<img
    title="Model comparison page: Performance"
    alt="A screenshot of model comparison"
    src={aiModelComparisonPerformance}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare performance between different AI models in your stack.
</figcaption>

Compare how one model performs within an app against the average performance across all services.

* Conduct A/B tests when testing different prompts during prompt engineering.
* Compare a model's performance and accuracy during one time window against another time window. From there, determine how a change made to your models affected performance over time. 
* Evaluate how a model performed during a specific time window when customer traffic peaked.

### Compare model cost [#compare-cost]

<img
    title="Model comparison page: Cost"
    alt="A screenshot of model comparison"
    src={aiModelComparisonCost}
/>

<figcaption>
    Go to **<DoNotTranslate>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>**: Compare cost between different AI models in your stack.
</figcaption>

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.

* **Tokens per completion**: The token average for all completion events.
* **Prompt tokens**: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
* **Completion tokens**: The number of tokens consumed by the model when it generates the response delivered to the end user.

When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.