---
title: 'Introduction to AI monitoring'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiTraceViewIntroPage from 'images/ai_screenshot-full_trace-view-intro-page.webp'

import aiAIResponsesOverview from 'images/ai_screenshot-full_AI-responses-overview.webp'

When New Relic talks about AI, we're referring to the layer of your environment that uses a large language model (LLM) to generate a response when it receives an end user prompt. Your app contains an AI assistant, but behind the AI are libraries, cost implications, vector databases, and new processes that can be monitored.   

<img
    title="Trace waterfall for AI monitoring"
    alt="A screenshot that shows the trace waterfall page for an individual AI response"
    src={aiTraceViewIntroPage}
/>

<figcaption>
    Go to **[one.newrelic.com](https://one.newrelic.com) > AI monitoring > AI responses**, then select a response row. 
</figcaption>

With AI monitoring, you can evaluate the performance of your AI by instrumenting your app with one of our APM agents. AI monitoring collects trace and span data about how your model generated a response, tracks performance metrics like latency and error rate, and gives you more oversight into your model's token usage. 

## What is AI Monitoring? [#what-is-ai-monitoring]

When your AI receives a prompt, an agent traces how the response generated through external LLMs, vector stores, and event data. From the UI, you can monitor performance trends through app metrics like uptime, latency, and error rate. With AI monitoring, you can:

* **Evaluate errors and bugs in your code.** Use the responses table to identify AI responses that contain errors. Dig into the methods and calls your AI-powered app makes when generating a response. 
* **Track token usage.** Did your prompt engineers make a recent change that affected cost? Use the AI response page to track tokens and keep costs down.
* **Compare models.** If you're using different models in different environments, compare their performance against how much they cost.

## Use AI monitoring to improve performance [#improve-performance]

<img
    title="AI responses page"
    alt="A screenshot of the AI responses page"
    src={aiAIResponsesOverview}
/>

<figcaption>
    To overview your AI-powered app's performance: Go to **[one.newrelic.com](https://one.newrelic.com) > AI monitoring > AI responses**.
</figcaption>

Our AI responses page lets you overview how well your AI performs. See the number of responses generated in a given time period, then check how quickly your AI delivers responses. Are your end users waiting too long for a response? Did token usage spike recently? You can use the AI responses page to see general trends, then drill down by individual response, service, or model to improve your AI-powered app's performance. 

## Get started with AI monitoring [#get-started-ai-monitoring]

Ready to get started? Confirm that your app compatibility, then [install AI monitoring](/docs/install/ai-monitoring).

<table>
    <thead>
        <tr>
            <th>
                Agent version
            </th>
            <th>
                Supported libraries
            </th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>
            [Go version 3.31.0 and above](/docs/apm/agents/go-agent/get-started/go-agent-compatibility-requirements/#digital-intelligence-platform)
            </td>
            <td>
            * [Go OpenAI library](https://github.com/sashabaranov/go-openai) versions 3.4.0 and above
            * [AWS SDK for Go v2](https://github.com/aws/aws-sdk-go-v2) versions 1.6.0 and above
            </td>
        </tr>
        <tr>
            <td>
            [Node.js version 11.13.0 and above](/docs/apm/agents/nodejs-agent/getting-started/compatibility-requirements-nodejs-agent/#digital-intelligence-platform)
            </td>
            <td>
            * [OpenAI Node.js API library](https://www.npmjs.com/package/openai/v/4.0.0-beta.4) versions 4.0.0 and above. If your model uses streaming, the Node.js agent supports versions 4.12.2 and above
            * [AWS SDK for JavaScript BedrockRuntime Client](https://www.npmjs.com/package/@aws-sdk/client-bedrock-runtime) versions 3.474.0 and above
            * [LangChain.js](https://www.npmjs.com/package/langchain/v/0.1.17) versions 0.1.17 and above
            </td>
        </tr>
        <tr>
            <td>
            [Python version 9.9.0 and above](/docs/apm/agents/python-agent/getting-started/compatibility-requirements-python-agent/#digital-intelligence-platform)
            </td>
            <td>
            * [OpenAI](https://pypi.org/project/openai/) library versions 1.13.3 and above
            * [Boto3 AWS SDK for Python](https://docs.aws.amazon.com/pythonsdk/)  versions ELEPHANT and above
            * [LangChain](https://pypi.org/project/langchain/0.0.300/) versions 0.1.17 and above
            </td>
        </tr>
        <tr>
            <td>
                [Ruby version 9.8.0 and above](/docs/apm/agents/ruby-agent/getting-started/ruby-agent-requirements-supported-frameworks/#digital-intelligence-platform)
            </td>
            <td>
                [`ruby_openai` gem](https://github.com/alexrudall/ruby-openai) version 3.4.0 and above
            </td>
        </tr>
    </tbody>
</table>