---
title: 'Customize the agent for AI monitoring'
metaDescription: 'You can apply certain configurations to your APM agents to change how your AI data appears in New Relic.'
freshnessValidatedDate: never
---

Once you [install AI monitoring](/install/ai-monitoring), you can configure the default behavior of the agent or update your app to collect different kinds of data. 

## Configure the agent [#configure-agents]

Update default agent behavior for AI monitoring at these agent configuration docs: 

* [Go](/docs/apm/agents/go-agent/configuration/go-agent-configuration/#ai-monitoring)
* [Node.js](/docs/apm/agents/nodejs-agent/installation-configuration/nodejs-agent-configuration/#ai-monitoring)
* [Python](/docs/apm/agents/python-agent/configuration/python-agent-configuration/#ai-monitoring)
* [Ruby](/docs/apm/agents/ruby-agent/configuration/ruby-agent-configuration/#ai-monitoring)

## Implement token count API [#enable-token]

<Callout variant="tip">
If you haven't disabled `ai_monitoring.record_content.enabled`, you don't need to implement the token count callback API.
</Callout>

Disabling `ai_monitoring.record_content.enabled` stops the agent from sending AI content to New Relic. This means the agent won't send token counts for interactions with your app. You can implement a callback in your app code to determine the token counts locally, then have that information forwarded to New Relic.

Refer to the below snippets as a reference guide for counting tokens locally:

<CollapserGroup>
    <Collapser
        id="go-token-api"
        title="Go"
    >

        INSERT_TEXT_HERE

    </Collapser>
    <Collapser
        id="nodejs-token-api"
        title="Node.js"
    >

    ```js
    const tiktoken = require('tiktoken')

    newrelic.setLlmTokenCountCallback(tokenCountCallback)

    /**
     * Calculate token counts locally based on the model being used and the content.
     * This callback will be invoked for each message as part of a communication with the LLM.
     * If the application supports more than one model, it may require finding libraries for
     * each model to support token counts appropriately.
     * 
     * @param {string} model - name of model (i.e. gpt-3.5-turbo)
     * @param {string} content - prompt or completion response
     * @returns {number} number of tokens that represent the supplied content for the given model
     */
    function tokenCountCallback(model, content) {
      if (model.includes('gpt')) {
        const enc = tiktoken.encoding_for_model(model)
        const count = enc.encoding(content).length()
        enc.free()

        return count
      }

      // for models that are not supported in the example
      return 0
    }
    ```

    </Collapser>
    <Collapser
        id="python-token-api"
        title="Python"
    >

    This callback will be invoked for each message as part of a communication with the LLM. If the application supports more than one model, it may require finding libraries for each model to support token counts appropriately.

    Refer to the following arguments when adapting this example:

      * `model`: Name of the model provided by or returned from the LLM
      * `content`: Context, input or output of the LLM

    ```py
        def token_count_callback(model, content):

        if "gpt" in model:
            import tiktoken

            enc = tiktoken.encoding_for_model(model)
            return len(enc.encode(content))

        # for models that are not supported in the example
        return 0

        # add this line where your application starts up
          newrelic.agent.set_llm_token_count_callback(token_count_callback)
    ```

    </Collapser>
    <Collapser
        id="ruby-token-api"
        title="Ruby"
    >
        ```ruby
          require 'tiktoken_ruby'

          def token_count_callback(model, content)
              if model.includes? "gpt"
                  enc = Tiktoken.encoding_for_model(model)
                  return enc.encode(content).length
              end

              # for models that are not supported in the example
              return 0
          end

          NewRelic::Agent.set_llm_token_count_callback(token_count_callback)
        ```
    </Collapser>
</CollapserGroup>

Keep in mind that: 

  * It's your responsibility to pick an appropriate tokenizer for your supported models and app languages.
  * The below snippets are examples for how you might use an LLM token count callback. That said, we do not provide support for setting up the token count API.

## Enable user feedback API [#enable-feedback]

AI monitoring can correlate trace IDs between a generated message from your AI and the feedback an end user submitted. When you update your code to correlate messages with feedback, you need to take the trace ID and pass it to the feedback call, as these two events occur at two different endpoints.

To pass the trace ID when recording a feedback event, refer to the below code samples for supported languages:

<CollapserGroup>
    <Collapser
        id="go-feedback-feature"
        title="Go"
    >

      ```go
      /* given app, a newrelic application value, and a chat completion message response resp of type nropenai.ChatCompletionResponseWrapper, */
      rating := "4"
      category := "informative"
      message := "The response was concise yet thorough."
      customMetadata := map[string]any{
        "llm.foo": "bar"
      }
      ```

    </Collapser>
    <Collapser
        id="nodejs-feedback-feature"
        title="Node.js"
    >

      ```js
      const responses = new Map()
      // This appears in a handler. It stores a reference to traceId via `newrelic.getTraceMetadata()
      fastify.post('/chat-completion', async(request, reply) => {
        const { message = 'Say this is a test', model = 'gpt-4' } = request.body || {}

        // Assigns conversation_id via custom attribute API
        const conversationId = uuid()
        newrelic.addCustomAttribute('llm.conversation_id', conversationId)

        const chatCompletion = await openai.chat.completions.create({
          temperature: 0.5,
          messages: [{ role: 'user', content: message }],
          model
        });

        const { traceId } = newrelic.getTraceMetadata()
        responses.set(chatCompletion.id, { traceId })
        return reply.send(chatCompletion)
      })

      // To post feedback. it'll get the traceId from some context and post it via `newrelic.recordLlmFeedback`
      fastify.post('/feedback', (request, reply) => {
        const { category = 'feedback-test', rating = 1, message = 'Good talk', metadata, id } = request.body || {}
        const { traceId } = responses.get(id)
        if (!traceId) {
          return reply.code(404).send(`No trace id found for ${message}`)
        }

        newrelic.recordLlmFeedbackEvent({
          traceId,
          category,
          rating,
          message,
          metadata
        })

        return reply.send('Feedback recorded')
      })
      ```   
      
</Collapser>
    <Collapser
        id="python-feedback-feature"
        title="Python"
    >
    
      ```python
      import newrelic.agent

      def message(request):
          trace_id = newrelic.agent.current_trace_id()

      def feedback(request):
          record_feedback(trace_id=request.trace_id, rating=request.rating)
      ```    
</Collapser>
    <Collapser
        id="ruby-feedback-feature"
        title="Ruby"
    >
        ```ruby
        trace_id = NewRelic::Agent::Tracer.current_trace_id
        
        NewRelic::Agent.record_llm_feedback_event(trace_id: trace_id, rating: rating)
        ```
    </Collapser>
</CollapserGroup>