---
title: 'Configure AI monitoring'
metaDescription: 'You can apply certain configurations to your APM agents to change how your AI data appears in New Relic.'
freshnessValidatedDate: never
---

Once you install AI monitoring, you can configure the default behavior of the agent or update your app to collect different kinds of data. 

## Configure the agent [#configure-agents]

Update default agent behavior for AI monitoring at these agent configuration docs: 

* [Go](/docs/apm/agents/go-agent/configuration/go-agent-configuration/#ai-monitoring)
* [Node.js](/docs/apm/agents/nodejs-agent/installation-configuration/nodejs-agent-configuration/#ai-monitoring)
* [Python](/docs/apm/agents/python-agent/configuration/python-agent-configuration/#ai-monitoring)
* [Ruby](/docs/apm/agents/ruby-agent/configuration/ruby-agent-configuration/#ai-monitoring)

## Enable token API [#enable-token]

If you've opted to drop message contents from LLM event data, you can still forward token count information to New Relic. This allows you to keep track of the amount of tokens your model uses without storing message contents between your end user and models. 

<Tabs>
	<TabsBar>
        <TabsBarItem id="nodejs-token">
            Node.js
        </TabsBarItem>
        <TabsBarItem id="python-token">
            Python
        </TabsBarItem>
    </TabsBar>

    <TabsPages>
        <TabsPageItem id="nodejs-token">
        ```js
            // register callback to set token counts 
            newrelic.setLlmTokenCount(callback)

            /**
             * Sample callback to calculate token count. Must be a synchronous method.
             * This callback is optional, and is designed for customers that cannot send
             * the content in the Llm events but still want token counts in the case
             * they are not present.
             * @param {string} model name of model
             * @param {string} content content of call
             * @returns {int} token count for content, customers must use relevant libraries to calculate this
             */
            function callback(model, content) {
            // put your business logic to calculate content
            return 10
            }
        ``` 
        </TabsPageItem>
        <TabsPageItem id="python-token">
        HERE
        </TabsPageItem>
    </TabsPages>
</Tabs>

## Enable user feedback [#enable-feedback]

AI monitoring can correlate trace IDs between a generated message from your AI and the feedback an end user submitted. 

<Tabs>
	<TabsBar>
        <TabsBarItem id="nodejs-feedback">
            Node.js
        </TabsBarItem>
        <TabsBarItem id="python-feedback">
            Python
        </TabsBarItem>
    </TabsBar>

    <TabsPages>
        <TabsPageItem id="nodejs-feedback">
```js
const responses = new Map()
// This would be in a handler. it'll store a reference to traceId via `newrelic.getTraceMetadata()
fastify.post('/chat-completion', async(request, reply) => {
  const { message = 'Say this is a test', model = 'gpt-4' } = request.body || {}

  // assign conversation_id via custom attribute API
  const conversationId = uuid()
  newrelic.addCustomAttribute('llm.conversation_id', conversationId)

  const chatCompletion = await openai.chat.completions.create({
    temperature: 0.5,
    messages: [{ role: 'user', content: message }],
    model
  });

  const { traceId } = newrelic.getTraceMetadata()
  responses.set(chatCompletion.id, { traceId })
  return reply.send(chatCompletion)
})

// To post feedback. it'll get the traceId from some context and post it via `newrelic.recordLlmFeedback`
fastify.post('/feedback', (request, reply) => {
  const { category = 'feedback-test', rating = 1, message = 'Good talk', metadata, id } = request.body || {}
  const { traceId } = responses.get(id)
  if (!traceId) {
    return reply.code(404).send(`No trace id found for ${message}`)
  }

  newrelic.recordLlmFeedbackEvent({
    traceId,
    category,
    rating,
    message,
    metadata
  })

  return reply.send('Feedback recorded')
})
```        
        </TabsPageItem>
        <TabsPageItem id="python-feedback">
The code snippet below takes the endpoints that store message and feedback details and connects these contents to a trace ID. to 

        </TabsPageItem>
    </TabsPages>
</Tabs>