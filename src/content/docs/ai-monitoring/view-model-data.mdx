---
title: 'View AI model data in New Relic'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiModelInventoryOverviewPage from 'images/ai_screenshot-full_model-inventory-overview-page.webp'

import aiModelInventoryPerformancePage from 'images/ai_screenshot-full_model-inventory-performance-page.webp'

import aiAiModelComparisonPage from 'images/ai_screenshot-full_ai-model-comparison-page.webp'

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelInventoryOverviewPage}
/>

<figcaption>
    Go to <DoNotTranslate>**[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DoNotTranslate>: View data about interactions with your AI model.
</figcaption>

SCREENSHOT, UI

## Model inventory page [#model-inventory]

<img
    title="Model inventory performance page"
    alt="A screenshot of the performance page in model inventory"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    <DoNotTranslate>Go to **[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance**: Scope your data to model performance metrics and time series</DoNotTranslate>
</figcaption>

INTRO: Focus on overview, highest level (models-focused, performance, kind of data). Differentiate model inventory by saying that model inventory takes a vertical look at AI data whereas response overview looks at it horizontally. 

IMAGE: Ask moran for that diagram, redo it to make it make sense

DIFFERENT SCREENSHOT, UI

UI Tab 1: Describe briefly, one sentence
UI Tab 2: Describe briefly, one sentence
UI Tab 3: Describe briefly, one sentence
UI Tab 4: Describe briefly, one sentence

Discuss an example scenario briefly about how model inventory can tell you about completions made to models in your entities. 

## Model comparison page [#model-comparison]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelComparisonPage}
/>

<figcaption>
    Go to <DoNotTranslate>**[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page gives you the flexibility to analyze performance depending on the use case you're testing for. You can:


Compare how one model performs within an app against the average performance across all services.
Conduct A/B tests when testing different prompts during prompt engineering. For example, comparing a model's performance and accuracy during one time window with one set of prompts against another time window with a second set of prompts.
Evaluate how a model performed during a specific time window when customer traffic peaked.

Keep in mind that this page scopes your model comparison data to a single account. If your organization has multiple accounts that own several AI-powered apps, you wouldn't be able to compare model data between those accounts.

### Understand model cost [#understand-model-cost]

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.
Tokens per completion: The token average for all completion events.
Prompt tokens: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
Completion tokens: The number of tokens consumed by the model when it generates the response delivered to the end user.
When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.



