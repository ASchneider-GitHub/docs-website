---
title: 'View AI model data in New Relic'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: never
---

import aiModelInventoryOverviewPage from 'images/ai_screenshot-full_model-inventory-overview-page.webp'

import aiModelInventoryPerformancePage from 'images/ai_screenshot-full_model-inventory-performance-page.webp'

import aiModelComparisonPage from 'images/ai_screenshot-full_ai-model-comparison-page.webp'

Your AI toolchain is made up of an app layer and a model layer. Your app layer includes all the complex processes that go into generating a response including the final response to your end user. Your model layer, on the other hand, 

While AI monitoring can help you improve how your AI app performs, the app itself is only as good as the model that drives its end user interactions.  

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelInventoryOverviewPage}
/>

<figcaption>
    Go to <DoNotTranslate>**[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DoNotTranslate>**: View data about interactions with your AI model.
</figcaption>

## Model inventory page [#model-inventory]

<img
    title="Model inventory performance page"
    alt="A screenshot of the performance page in model inventory"
    src={aiModelInventoryPerformancePage}
/>

<figcaption>
    <DoNotTranslate>Go to **[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance**: Scope your data to model performance metrics and time series</DoNotTranslate>
</figcaption>

The model inventory page surfaces AI model data from the AI app layer. From the overview page, you can explore the number of requests made to a model against its response times, or you can analyze time series graphs that show you these metrics over time.

By scoping to the model itself, you can get a better sense of how the model processes and returns certain outputs. For example, your model may process several completions and make several calls to other services and databases in your toolchain before your AI app posts its response to your end user. With the model inventory page, you can see performance metrics for those model-specific functions.

### Analyze all your models [#analyze-all]

The model inventory page segments your data across errors, performance, and cost. Each page has similar graphs, tables, and time series as the overview page, but specific to a particular area you want to target.

* **Errors**: The errors page exposes common errors specific to your particular model. For example, lets say your model has an error code for exceeding your current quota. The errors page will tell you when it happened and its frequency across your models.
* **Performance**: Similar to metrics found in the AI app itself, the performance page tells you how many requests a model is processing and how long it takes to process. 
* **Cost**: If you use different models in your AI toolchain for different processes, compare how much each costs. Track the total tokens each model uses, or see the ratio of prompt tokens against completion tokens. 

## Model comparison page [#model-comparison]

<img
    title="Model inventory overview"
    alt="A screenshot of the overview page when you go to Model inventory"
    src={aiModelComparisonPage}
/>

<figcaption>
    Go to <DoNotTranslate>**[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DoNotTranslate>: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page gives you the flexibility to analyze performance depending on the use case you're testing for. You can:


Compare how one model performs within an app against the average performance across all services.
Conduct A/B tests when testing different prompts during prompt engineering. For example, comparing a model's performance and accuracy during one time window with one set of prompts against another time window with a second set of prompts.
Evaluate how a model performed during a specific time window when customer traffic peaked.

Keep in mind that this page scopes your model comparison data to a single account. If your organization has multiple accounts that own several AI-powered apps, you wouldn't be able to compare model data between those accounts.

### Compare model cost [#understand-model-cost]

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.
Tokens per completion: The token average for all completion events.
Prompt tokens: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
Completion tokens: The number of tokens consumed by the model when it generates the response delivered to the end user.
When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.



